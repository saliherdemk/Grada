{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d116f72d-9005-42e1-8040-b609ac724743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz\n",
    "# !pip install torch\n",
    "# !pip install tensorflow\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea9f453-183a-41ac-8258-10244e935212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import random\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, value, children=(), op = \"\"):\n",
    "        self.data = value\n",
    "        self.children = set(children)\n",
    "        self.label = \"\"\n",
    "        self.op = op\n",
    "        self.grad = 0.0\n",
    "        self.backward = lambda: None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Value({self.data})\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value({self.data})\"\n",
    "\n",
    "    def __add__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        output = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * output.grad\n",
    "            other.grad += 1.0 * output.grad\n",
    "\n",
    "        output.backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __radd__(self, other) -> Value:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        output = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output.grad * other.data\n",
    "            other.grad += output.grad * self.data\n",
    "\n",
    "        output.backward = _backward\n",
    "        return output\n",
    "    \n",
    "    def __rmul__(self, other) -> Value:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        other = other if isinstance(other, (int, float)) else other.data\n",
    "\n",
    "        output = Value(self.data ** other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * output.grad\n",
    "    \n",
    "        output.backward = _backward\n",
    "        return output\n",
    "    \n",
    "    def __rpow__(self, other) -> Value:\n",
    "        return self.__pow__(other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def __rtruediv__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        return other * (self ** -1)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other) -> Value:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def exp(self):\n",
    "        data = self.data\n",
    "        output = Value(math.exp(data), (self, ), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += output.data * output.grad\n",
    "            \n",
    "        output.backward = _backward\n",
    "        return output\n",
    "\n",
    "    def set_label(self, label):\n",
    "        self.label = label\n",
    "\n",
    "    def set_grad(self, grad):\n",
    "        self.grad = grad\n",
    "\n",
    "    def backprop(self, initial = True):\n",
    "        # if initial: self.set_grad(1.0)\n",
    "        # self.backward()\n",
    "        # for child in self.children:\n",
    "        #     child.backprop(False)\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "    \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "    \n",
    "        build_topo(self)\n",
    "        self.set_grad(1.0)\n",
    "        for node in reversed(topo):\n",
    "            node.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2346a6e1-71e3-4cce-bdb6-ae445d816eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class ActivationFunction(enum.Enum):\n",
    "    TANH = \"tanh\"\n",
    "    RELU = \"relu\"\n",
    "    SIGMOID = \"sigmoid\"\n",
    "\n",
    "def tanh(x: Value) -> Value:\n",
    "    e = (2*x).exp()\n",
    "    output = (e - 1) / (e + 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "def relu(x: Value) -> Value:\n",
    "    data = x.data\n",
    "    output = Value(max(0, data), (x, ), 'relu')\n",
    "    \n",
    "    def _backward():\n",
    "        x.grad += int(data > 0)\n",
    "        \n",
    "    output.backward = _backward\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x: Value) -> Value:\n",
    "    output = 1 / (1 + (-x).exp())\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    ActivationFunction.TANH: tanh,\n",
    "    ActivationFunction.RELU: relu,\n",
    "    ActivationFunction.SIGMOID: sigmoid,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1eeecdc-5a89-4149-9a30-0dd507184e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0); x1.set_label(\"x1\")\n",
    "x2 = Value(0.0);x2.set_label(\"x2\")\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0);w1.set_label(\"w1\")\n",
    "w2 = Value(1.0);w2.set_label(\"w2\")\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432)\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "n.backprop()\n",
    "\n",
    "\"\"\"\n",
    "o = relu(n)\n",
    "\n",
    "o.backprop()\n",
    "o.grad\n",
    "w1.grad\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1dc80e40-0e9e-42cf-985b-b79a8820b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Value(0.9999997749296758) X grad:  9.002811952251134e-07 2.2507029880627834e-07 W grad: 1.35042179283767e-06 0.0 9.002811952251134e-07 0.0 B grad: 4.501405976125567e-07 0.0\n",
      "Y Value(1.0) X grad:  -0.9999990997188047 3.000000225070299 W grad: 1.35042179283767e-06 3.0 9.002811952251134e-07 2.0 B grad: 4.501405976125567e-07 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.9999990997188047"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs \n",
    "x1 = Value(3.0)\n",
    "x2 = Value(2.0)\n",
    "# weights\n",
    "w1 = Value(2.0)\n",
    "w2 = Value(-1.0)\n",
    "w3 = Value(0.5)\n",
    "w4 = Value(3)\n",
    "\n",
    "# bias of the neuron\n",
    "b1 = Value(1)\n",
    "b2 = Value(-2)\n",
    "# x1*w1 + x2*w2 + b1\n",
    "x1w1 = x1*w1\n",
    "x2w3 = x2*w3\n",
    "x1w1x2w3 = x1w1 + x2w3\n",
    "y1 = x1w1x2w3 + b1\n",
    "y1 = tanh(y1)\n",
    "\n",
    "y1.backprop()\n",
    "print(\"Y\", y1,\"X grad: \",x1.grad, x2.grad, \"W grad:\", w1.grad, w2.grad,w3.grad,w4.grad, \"B grad:\", b1.grad,b2.grad)\n",
    "\n",
    "\n",
    "\n",
    "# 2*w3 + x2*w4 + b2\n",
    "x1w2 = x1*w2\n",
    "x2w4 = x2*w4\n",
    "x1w2x2w4 = x1w2 + x2w4\n",
    "y2 = x1w2x2w4 + b2\n",
    "\n",
    "y2.backprop()\n",
    "print(\"Y\", y2,\"X grad: \",x1.grad, x2.grad, \"W grad:\", w1.grad, w2.grad,w3.grad,w4.grad, \"B grad:\", b1.grad,b2.grad)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "o = relu(n)\n",
    "\n",
    "o.backprop()\n",
    "o.grad\n",
    "w1.grad\n",
    "\"\"\"\n",
    "x1.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c7e6cb-255e-4e15-9261-dc69f95b0a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(0.01896064643202053)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "        self.act_func = act_func=ActivationFunction.TANH\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        act = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        output = activation_functions[self.act_func](act)\n",
    "        return output\n",
    "      \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def change_act_func(act_func):\n",
    "        self.act_func = act_func\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout, act_func=ActivationFunction.RELU):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        self.act_func = act_func\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    def change_act_func(act_func):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.change_act_func(self.act_func)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "x = [2.0, 3.0, -1.0]\n",
    "m = MLP(3, [4, 4, 1])\n",
    "a = m(x)\n",
    "a.backprop()\n",
    "a\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d915f00-2cbd-4c8d-ad96-423b69b4460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3023a38f-1b59-4848-8479-514feb2597c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.010261702571224478 [Value(0.9575766431878318), Value(-0.954548934735842), Value(-0.9428388430098197), Value(0.9440646429714697)]\n",
      "1 0.00998794112079448 [Value(0.9581620893227643), Value(-0.9552028358850162), Value(-0.9435660915692748), Value(0.9448097978129645)]\n",
      "2 0.009727986021251061 [Value(0.958724997873161), Value(-0.955831368278779), Value(-0.9442667607550945), Value(0.9455270871365797)]\n",
      "3 0.009480830625461062 [Value(0.9592667592991462), Value(-0.956436097272486), Value(-0.9449424201187065), Value(0.9462181907683457)]\n",
      "4 0.009245563216291225 [Value(0.9597886475942699), Value(-0.95701845763505), Value(-0.9455945129687836), Value(0.9468846505158861)]\n",
      "5 0.009021356117228126 [Value(0.9602918324938238), Value(-0.9575797671412575), Value(-0.9462243690956291), Value(0.9475278843996499)]\n",
      "6 0.008807456264447492 [Value(0.9607773901575866), Value(-0.9581212384815246), Value(-0.946833215965974), Value(0.9481491991310143)]\n",
      "7 0.008603177016927501 [Value(0.9612463125470552), Value(-0.9586439897290289), Value(-0.947422188600412), Value(0.9487498010869845)]\n",
      "8 0.008407891019345304 [Value(0.9616995156814035), Value(-0.9591490535656132), Value(-0.9479923383124865), Value(0.9493308059910338)]\n",
      "9 0.008221023963472384 [Value(0.9621378469270712), Value(-0.9596373854361879), Value(-0.948544640461005), Value(0.9498932474766086)]\n",
      "10 0.00804204911905968 [Value(0.9625620914517173), Value(-0.9601098707751938), Value(-0.9490800013443693), Value(0.9504380846826019)]\n",
      "11 0.007870482525918562 [Value(0.9629729779533024), Value(-0.9605673314269983), Value(-0.9495992643467356), Value(0.9509662090075273)]\n",
      "12 0.007705878755954895 [Value(0.9633711837584663), Value(-0.9610105313640422), Value(-0.9501032154299313), Value(0.9514784501303527)]\n",
      "13 0.007547827168004656 [Value(0.9637573393705497), Value(-0.9614401817914594), Value(-0.9505925880517357), Value(0.9519755813902764)]\n",
      "14 0.0073959485900128965 [Value(0.9641320325360224), Value(-0.9618569457142399), Value(-0.9510680675798985), Value(0.9524583246045734)]\n",
      "15 0.007249892372834863 [Value(0.9644958118883743), Value(-0.9622614420323512), Value(-0.9515302952617809), Value(0.9529273543925835)]\n",
      "16 0.007109333768079218 [Value(0.9648491902203205), Value(-0.9626542492202396), Value(-0.9519798718014577), Value(0.953383302064565)]\n",
      "17 0.006973971589239022 [Value(0.9651926474282547), Value(-0.963035908639505), Value(-0.9524173605892746), Value(0.9538267591262309)]\n",
      "18 0.006843526121104329 [Value(0.9655266331669948), Value(-0.9634069275270759), Value(-0.9528432906230117), Value(0.9542582804430466)]\n",
      "19 0.006717737247299339 [Value(0.9658515692478739), Value(-0.9637677816956738), Value(-0.9532581591548138), Value(0.954678387102647)]\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "  \n",
    "    ypred = [m(x) for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "    \n",
    "    for p in m.parameters():\n",
    "        p.set_grad(0.0)\n",
    "    \n",
    "    loss.backprop()\n",
    "    \n",
    "    for p in m.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "    \n",
    "    print(k, loss.data, ypred)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3f0ea4-a053-458f-b9ce-f69beea0a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a8c030-339b-4b9f-9565-d9588f1ad7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 18:17:15.750703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-12 18:17:15.859656: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-12 18:17:15.888870: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-12 18:17:16.094665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-12 18:17:17.230962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8813735870195432\n",
      "---\n",
      "x2 1.0\n",
      "w2 0.0\n",
      "x1 -3.0\n",
      "w1 2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.Variable([2.0], dtype=tf.float64)\n",
    "x2 = tf.Variable([0.0], dtype=tf.float64)\n",
    "w1 = tf.Variable([-3.0], dtype=tf.float64)\n",
    "w2 = tf.Variable([1.0], dtype=tf.float64)\n",
    "b = tf.Variable([6.8813735870195432], dtype=tf.float64)\n",
    "\n",
    "n = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "print(n.numpy().item())\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    n = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "grads = tape.gradient(n, [x1, x2, w1, w2, b])\n",
    "\n",
    "print('---')\n",
    "print('x2', grads[1].numpy().item())\n",
    "print('w2', grads[3].numpy().item())\n",
    "print('x1', grads[0].numpy().item())\n",
    "print('w1', grads[2].numpy().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42de596-dee4-4857-a17a-b1b2f564874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class XORNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(in_features=2, out_features=2)  # Hidden layer\n",
    "        self.output = nn.Linear(in_features=2, out_features=1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x)) \n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = XORNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0da05341-6e50-4f33-a976-a51694009108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7440416216850281\n",
      "Epoch 1000, Loss: 0.025409312918782234\n",
      "Epoch 2000, Loss: 0.010081389918923378\n",
      "Epoch 3000, Loss: 0.006137249059975147\n",
      "Epoch 4000, Loss: 0.004370155744254589\n",
      "Epoch 5000, Loss: 0.003379428992047906\n",
      "Epoch 6000, Loss: 0.0027468805201351643\n",
      "Epoch 7000, Loss: 0.0023107770830392838\n",
      "Epoch 8000, Loss: 0.0019910151604562998\n",
      "Epoch 9000, Loss: 0.0017478432273492217\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# XOR input and output data\n",
    "inputs = torch.tensor([[0.0, 0.0],\n",
    "                       [0.0, 1.0],\n",
    "                       [1.0, 0.0],\n",
    "                       [1.0, 1.0]], dtype=torch.float32)\n",
    "targets = torch.tensor([[0.0], [1.0], [1.0], [0.0]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ae133-cb50-4294-88ba-2012397209a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
