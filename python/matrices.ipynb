{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "675f2dfd-e596-4340-bb40-fc71d85542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            tanh_row = [math.tanh(_x) for _x in row]\n",
    "            result.append(tanh_row)\n",
    "        output = Tensor(result)\n",
    "        output._prev = [x]\n",
    "\n",
    "        def _backward():\n",
    "            if x.grad is None:\n",
    "                x.grad = [[0 for _ in range(len(x.data[0]))] for _ in range(len(x.data))]\n",
    "            for i in range(len(x.data)):\n",
    "                for j in range(len(x.data[0])):\n",
    "                    x.grad[i][j] += (1 - math.tanh(x.data[i][j])**2) * output.grad[i][j]\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            relu_row = [max(0, _x) for _x in row]\n",
    "            result.append(relu_row)\n",
    "        output = Tensor(result)\n",
    "        output._prev = [x]\n",
    "\n",
    "        def _backward():\n",
    "            if x.grad is None:\n",
    "                x.grad = [[0 for _ in range(len(x.data[0]))] for _ in range(len(x.data))]\n",
    "            for i in range(len(x.data)):\n",
    "                for j in range(len(x.data[0])):\n",
    "                    x.grad[i][j] += (1 if x.data[i][j] > 0 else 0) * output.grad[i][j]\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            sigmoid_row = [1 / (1 + math.exp(-_x)) for _x in row]\n",
    "            result.append(sigmoid_row)\n",
    "        output = Tensor(result)\n",
    "        output._prev = [x]\n",
    "\n",
    "        def _backward():\n",
    "            if x.grad is None:\n",
    "                x.grad = [[0 for _ in range(len(x.data[0]))] for _ in range(len(x.data))]\n",
    "            for i in range(len(x.data)):\n",
    "                for j in range(len(x.data[0])):\n",
    "                    sig = 1 / (1 + math.exp(-x.data[i][j]))\n",
    "                    x.grad[i][j] += sig * (1 - sig) * output.grad[i][j]\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        max_val = [[max(row)] for row in x.data]\n",
    "        exps = [[math.exp(i - max_row[0]) for i in row] for row, max_row in zip(x.data, max_val)]\n",
    "        \n",
    "        sums = [[sum(row)] for row in exps]\n",
    "        softmax_result = [[exps[i][j] / sums[i][0] for j in range(len(exps[i]))] for i in range(len(exps))]\n",
    "        \n",
    "        result = Tensor(softmax_result)\n",
    "        \n",
    "        result._prev = [x]\n",
    "        \n",
    "        def _backward():\n",
    "            if x.grad is None:\n",
    "                x.grad = [[0 for _ in range(len(x.data[0]))] for _ in range(len(x.data))]\n",
    "\n",
    "            for i in range(len(result.data)):\n",
    "                for j in range(len(result.data[0])):\n",
    "                    for k in range(len(result.data[0])):\n",
    "                        grad_val = result.data[i][j] * ((1 if j == k else 0) - result.data[i][k])\n",
    "                        x.grad[i][k] += result.grad[i][j] * grad_val\n",
    "        \n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    \"tanh\": ActivationFunctions.tanh,\n",
    "    \"relu\": ActivationFunctions.relu,\n",
    "    \"sigmoid\": ActivationFunctions.sigmoid,\n",
    "    \"softmax\": ActivationFunctions.softmax,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "164d7481-5b33-453d-8571-fbb731cb6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    @staticmethod\n",
    "    def mse(output, target):\n",
    "        squared_diffs = [[(o - t) ** 2 for o, t in zip(out_row, target_row)] \n",
    "                          for out_row, target_row in zip(output.data, target.data)]\n",
    "        \n",
    "        loss_value = sum(sum(row) for row in squared_diffs) / (len(output.data) * len(output.data[0]))\n",
    "\n",
    "        loss_tensor = Tensor([[loss_value]])\n",
    "        loss_tensor._prev = [output] \n",
    "\n",
    "        def _backward():\n",
    "            if output.grad is None:\n",
    "                output.grad = [[0.0 for _ in range(len(output.data[0]))] for _ in range(len(output.data))]\n",
    "            for i in range(len(output.data)):\n",
    "                for j in range(len(output.data[0])):\n",
    "                    output.grad[i][j] += (2 * (output.data[i][j] - target.data[i][j])) / (len(output.data) * len(output.data[0]))\n",
    "\n",
    "        loss_tensor._backward = _backward  \n",
    "\n",
    "        return loss_tensor\n",
    "\n",
    "err_functions = {\n",
    "    \"mse\": ErrorFunction.mse,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "f79c48a1-1968-42bf-bf04-149fd156a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        self.set_data(data)\n",
    "        self.grad = None\n",
    "        self._backward = lambda: None\n",
    "        self._prev = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, prev={self._prev} shape={self.shape})\"\n",
    "\n",
    "    def set_data(self,data):\n",
    "        self.data = data\n",
    "        self.determine_shape()\n",
    "\n",
    "    def determine_shape(self):\n",
    "        data = self.data\n",
    "        if isinstance(data[0], list):\n",
    "            rows = len(data)\n",
    "            cols = len(data[0]) if rows > 0 else 0\n",
    "            self.shape = (rows, cols)\n",
    "        else:\n",
    "            self.shape = (len(data), 1)\n",
    "\n",
    "    def dot(self, other):\n",
    "        v1 = self.data  \n",
    "        v2 = other.data \n",
    "        \n",
    "        m = len(v1)      \n",
    "        n = len(v2[0])  \n",
    "        p = len(v2)    \n",
    "    \n",
    "        if len(v1[0]) != len(v2):\n",
    "            raise ValueError(\"Incompatible dimensions for matrix multiplication.\")\n",
    "        \n",
    "        result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    \n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                for k in range(p):\n",
    "                    result[i][j] += v1[i][k] * v2[k][j]        \n",
    "        output = Tensor(result)\n",
    "        \n",
    "        def _backward():\n",
    "            \n",
    "            if self.grad is None:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            if other.grad is None:\n",
    "                other.grad = [[0 for _ in range(len(other.data[0]))] for _ in range(len(other.data))]\n",
    "            \n",
    "            for i in range(m):\n",
    "                for k in range(p):\n",
    "                    for j in range(n):\n",
    "                        self.grad[i][k] += output.grad[i][j] * other.data[k][j]\n",
    "            \n",
    "            self_T = self.transpose()\n",
    "            for k in range(p):\n",
    "                for j in range(n):\n",
    "                    for i in range(m):\n",
    "                        other.grad[k][j] += output.grad[i][j] * self_T.data[k][i]\n",
    "        \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "\n",
    "    # FIXME: broadcast value replacement causes error\n",
    "    def broadcast(self, m):\n",
    "        result = []\n",
    "        for _ in range(m):\n",
    "            result.append(self.data)\n",
    "        self.set_data(result)\n",
    "\n",
    "    def add(self, other):\n",
    "        if self.shape != other.shape or not isinstance(other.data[0], list):\n",
    "            other.broadcast(self.shape[0])\n",
    "        \n",
    "        result = []\n",
    "        for i in range(len(self.data)):\n",
    "            row = []\n",
    "            for j in range(len(self.data[0])):\n",
    "                row.append(self.data[i][j] + other.data[i][j])\n",
    "            result.append(row)\n",
    "        output = Tensor(result)\n",
    "\n",
    "        def _backward():\n",
    "            if self.grad is None:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            for i in range(len(self.data)):\n",
    "                for j in range(len(self.data[0])):\n",
    "                    self.grad[i][j] += output.grad[i][j]\n",
    "    \n",
    "            if other.grad is None:\n",
    "                other.grad = [[0 for _ in range(len(other.data[0]))] for _ in range(len(other.data))]\n",
    "            for i in range(len(other.data)):\n",
    "                for j in range(len(other.data[0])):\n",
    "                    other.grad[i][j] += output.grad[i][j]\n",
    "    \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def step(self, lr):\n",
    "        for i in range(len(self.data)):\n",
    "            row = []\n",
    "            for j in range(len(self.data[0])):\n",
    "                self.data[i][j] += self.grad[i][j] * -lr\n",
    "\n",
    "\n",
    "    def transpose(self):\n",
    "        transposed_data = [[self.data[j][i] for j in range(self.shape[0])]\n",
    "                           for i in range(self.shape[1])]\n",
    "        return Tensor(transposed_data)\n",
    "\n",
    "    def backward(self):\n",
    "        if self.grad is None:\n",
    "            self.grad = [[1.0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "        self._backward()\n",
    "        for prev_tensor in self._prev:\n",
    "            prev_tensor.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "766ff2ec-899b-4222-8982-78da68868609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self,nin,nout,act_func = \"tanh\"):\n",
    "        self.weights = Tensor([[i + 1 for i in range(nout)] for _ in range(nin)])\n",
    "        self.biases = Tensor([1 for _ in range(nout)])\n",
    "        \n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = Tensor(weights)\n",
    "        self.biases = Tensor(biases)\n",
    "        \n",
    "    def set_biases(self, biases):\n",
    "        self.biases = Tensor(biases)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        result = inputs.dot(self.weights).add(self.biases)\n",
    "        return result\n",
    "        \n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activations = []):\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(DenseLayer(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if(i < len(activations)):\n",
    "                self.activations.append(None if activations[i] == None else activations[i])\n",
    "            else:\n",
    "                self.activations.append(None)\n",
    "        \n",
    "            \n",
    "    def set_parameters(self,weights, biases):\n",
    "        for layer,weight, bias in zip(self.layers,weights, biases):\n",
    "            layer.set_parameters(weight, bias)\n",
    "            \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            inputs = layer.forward(inputs)\n",
    "            if self.activations[idx]:\n",
    "                inputs = activation_functions[self.activations[idx]](inputs)\n",
    "            \n",
    "        return inputs\n",
    "\n",
    "    def get_parameters(self):\n",
    "        all_weights = []\n",
    "        all_biases = []\n",
    "        for layer in self.layers:\n",
    "            weights, biases = layer.get_parameters()\n",
    "            all_weights.append(weights)\n",
    "            all_biases.append(biases)\n",
    "            \n",
    "        return all_weights, all_biases\n",
    "\n",
    "    def step(self, lr):\n",
    "        weights, biases = self.get_parameters()\n",
    "        for param in weights + biases:\n",
    "            param.step(lr)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        weights, biases = self.get_parameters()\n",
    "        for param in weights + biases:\n",
    "            param.grad = None\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "651420b6-bb03-4e51-aaa6-a09f8deca542",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[623], line 147\u001b[0m\n\u001b[1;32m    140\u001b[0m             mlp\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Test Passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[43mcompare_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m  \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m  \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m  \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m  \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[623], line 137\u001b[0m, in \u001b[0;36mcompare_mlp\u001b[0;34m(layer_sizes, x, y, batch_size, learning_rate, epochs)\u001b[0m\n\u001b[1;32m    135\u001b[0m mlp\u001b[38;5;241m.\u001b[39mstep(learning_rate)\n\u001b[1;32m    136\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 137\u001b[0m \u001b[43mcompare_weights_and_biases_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    140\u001b[0m mlp\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[623], line 84\u001b[0m, in \u001b[0;36mcompare_weights_and_biases_grads\u001b[0;34m(mlp_torch, mlp)\u001b[0m\n\u001b[1;32m     82\u001b[0m expected \u001b[38;5;241m=\u001b[39m e_biases[i]\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     83\u001b[0m current \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(biases[i]\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m, \\\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights mismatch in layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProvided: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class MLP_TORCH(nn.Module):\n",
    "    def __init__(self, layer_sizes, activations = []):\n",
    "        super(MLP_TORCH, self).__init__()\n",
    "        layers = []\n",
    "        self.activations = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if(i < len(activations)):\n",
    "                self.activations.append(None if activations[i] == None else activations[i])\n",
    "            else:\n",
    "                self.activations.append(None)\n",
    "        self.network = nn.ModuleList(layers)\n",
    "        \n",
    "    def get_act_func(self, name):\n",
    "        if(name == \"tanh\"):\n",
    "            return nn.Tanh\n",
    "        if(name == \"relu\"):\n",
    "            return nn.ReLU\n",
    "        if(name == \"sigmoid\"):\n",
    "            return nn.Sigmoid\n",
    "        if(name == \"softmax\"):\n",
    "            return nn.Softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            x = layer(x)\n",
    "            if self.activations[idx]:\n",
    "                act_func = self.get_act_func(self.activations[idx])()\n",
    "                x = act_func(x)\n",
    "        return x\n",
    "\n",
    "    def set_parameters(self, weights, biases):\n",
    "        with torch.no_grad():\n",
    "            for idx, layer in enumerate(self.network):\n",
    "                layer.weight = nn.Parameter(torch.tensor(weights[idx], dtype=torch.float32).T)\n",
    "                layer.bias = nn.Parameter(torch.tensor(biases[idx], dtype=torch.float32))\n",
    "                \n",
    "    def get_parameters(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            weights.append(layer.weight)\n",
    "            biases.append(layer.bias)\n",
    "        return weights, biases\n",
    "\n",
    "def get_err_func(name):\n",
    "    if(name == \"mse\"):\n",
    "        return nn.MSELoss\n",
    "\n",
    "def compare_weights_and_biases_values(mlp_torch, mlp):\n",
    "    e_weights, e_biases = mlp_torch.get_parameters()\n",
    "    weights, biases = mlp.get_parameters()\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        expected = e_weights[i].T\n",
    "        current = torch.tensor(weights[i].data)\n",
    "        assert torch.allclose(expected, current, rtol=1e-4, atol=1e-4), \\\n",
    "            f\"Weights mismatch in layer {i}:\\nExpected: {expected}\\nProvided: {current}\"\n",
    "    for i in range(len(biases)):\n",
    "        expected = e_biases[i].flatten()\n",
    "        current = torch.tensor(biases[i].data).flatten()\n",
    "        assert torch.allclose(expected, current, rtol=1e-4, atol=1e-4), \\\n",
    "            f\"Weights mismatch in layer {i}:\\nExpected: {expected}\\nProvided: {current}\"\n",
    "\n",
    "    #print(\"Parameter values Match\")\n",
    "        \n",
    "def compare_weights_and_biases_grads(mlp_torch, mlp):\n",
    "    e_weights, e_biases = mlp_torch.get_parameters()\n",
    "    weights, biases = mlp.get_parameters()\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        expected = e_weights[i].grad.T\n",
    "        current = torch.tensor(weights[i].grad)\n",
    "        assert torch.allclose(expected, current, rtol=1e-4, atol=1e-4), \\\n",
    "            f\"Weights grads mismatch in layer {i}:\\nExpected: {expected}\\nProvided: {current}\"\n",
    "\n",
    "    for i in range(len(biases)):\n",
    "        expected = e_biases[i].grad.flatten()\n",
    "        current = torch.tensor(biases[i].grad).flatten()\n",
    "        assert torch.allclose(expected, current, rtol=1e-4, atol=1e-4), \\\n",
    "            f\"Weights mismatch in layer {i}:\\nExpected: {expected}\\nProvided: {current}\"\n",
    "\n",
    "    # print(\"Parameter grads Match\")\n",
    "\n",
    "def compare_outputs(o1,o2,i):\n",
    "    if o1.dtype != torch.float32:\n",
    "        o1 = o1.float()\n",
    "    if o2.dtype != torch.float32:\n",
    "        o2 = o2.float()\n",
    "    assert torch.allclose(o1, o2, rtol=1e-4, atol=1e-4), \\\n",
    "        f\"Output mismatch at batch starting index {i}:\\nMLP Output: {o1}\\nTorch Output: {o2}\"\n",
    "    #print(f\"Outputs match for this batch {i}\")\n",
    "\n",
    "def compare_mlp(layer_sizes, x, y, batch_size=2, learning_rate=0.001, epochs=1000):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    activations = [\"tanh\", \"relu\", \"softmax\"]\n",
    "    err_func = \"mse\"\n",
    "    for l in range(len(layer_sizes) - 1):\n",
    "        weights.append([[random.uniform(-1, 1) for _ in range(layer_sizes[l + 1])] for _ in range(layer_sizes[l])])\n",
    "        biases.append([random.uniform(-1, 1) for _ in range(layer_sizes[l + 1])])\n",
    "    mlp = MLP(layer_sizes, activations)\n",
    "    mlp.set_parameters(weights, biases)\n",
    "\n",
    "    mlp_torch = MLP_TORCH(layer_sizes, activations)\n",
    "    mlp_torch.set_parameters(weights, biases)\n",
    "\n",
    "    compare_weights_and_biases_values(mlp_torch, mlp)\n",
    "\n",
    "    optimizer = torch.optim.SGD(mlp_torch.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "\n",
    "            mlp_output = mlp.forward(Tensor(x_batch))\n",
    "            \n",
    "            torch_output = mlp_torch.forward(torch.tensor(x_batch))\n",
    "            \n",
    "            compare_outputs(torch.tensor(mlp_output.data), torch_output.data, i)\n",
    "\n",
    "            torch_loss_func = get_err_func(err_func)()\n",
    "            torch_loss = torch_loss_func(torch_output, torch.tensor(y_batch))\n",
    "            loss = err_functions[err_func](mlp_output, Tensor(y_batch))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch_loss.backward()\n",
    "            \n",
    "            mlp.step(learning_rate)\n",
    "            optimizer.step()\n",
    "            compare_weights_and_biases_grads(mlp_torch, mlp)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            mlp.zero_grad()\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} Test Passed\")\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "compare_mlp([3,3,2],[\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "],[[1.0, 3.0], [-1.0, -2.0], [-1.0, 1.0], [1.0, -1.0]])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db938989-d4bd-4472-8c7e-4485fe180c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b57a7-6743-4805-a4de-c73b11f97f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
