{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "675f2dfd-e596-4340-bb40-fc71d85542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            tanh_row = [math.tanh(_x) for _x in row]\n",
    "            result.append(tanh_row)\n",
    "        output = Tensor(result)\n",
    "        output._prev = [x]\n",
    "\n",
    "        def _backward():\n",
    "            if x.grad is None:\n",
    "                x.grad = [[0 for _ in range(len(x.data[0]))] for _ in range(len(x.data))]\n",
    "            for i in range(len(x.data)):\n",
    "                for j in range(len(x.data[0])):\n",
    "                    x.grad[i][j] += (1 - math.tanh(x.data[i][j])**2) * output.grad[i][j]\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            relu_row = [max(0, _x) for _x in row]\n",
    "            result.append(relu_row)\n",
    "        output = Tensor(result)\n",
    "        output._prev = [x]\n",
    "\n",
    "        def _backward():\n",
    "            if x.grad is None:\n",
    "                x.grad = [[0 for _ in range(len(x.data[0]))] for _ in range(len(x.data))]\n",
    "            for i in range(len(x.data)):\n",
    "                for j in range(len(x.data[0])):\n",
    "                    x.grad[i][j] += (1 if x.data[i][j] > 0 else 0) * output.grad[i][j]\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            sigmoid_row = [1 / (1 + math.exp(-_x)) for _x in row]\n",
    "            result.append(sigmoid_row)\n",
    "        output = Tensor(result)\n",
    "        output._prev = [x]\n",
    "\n",
    "        def _backward():\n",
    "            if x.grad is None:\n",
    "                x.grad = [[0 for _ in range(len(x.data[0]))] for _ in range(len(x.data))]\n",
    "            for i in range(len(x.data)):\n",
    "                for j in range(len(x.data[0])):\n",
    "                    sig = 1 / (1 + math.exp(-x.data[i][j]))\n",
    "                    x.grad[i][j] += sig * (1 - sig) * output.grad[i][j]\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        max_val = [[max(row)] for row in x.data]\n",
    "        exps = [[math.exp(i - max_row[0]) for i in row] for row, max_row in zip(x.data, max_val)]\n",
    "        \n",
    "        sums = [[sum(row)] for row in exps]\n",
    "        softmax_result = [[exps[i][j] / sums[i][0] for j in range(len(exps[i]))] for i in range(len(exps))]\n",
    "        \n",
    "        result = Tensor(softmax_result)\n",
    "        \n",
    "        result._prev = [x]\n",
    "        \n",
    "        def _backward():\n",
    "            if x.grad is None:\n",
    "                x.grad = [[0 for _ in range(len(x.data[0]))] for _ in range(len(x.data))]\n",
    "\n",
    "            for i in range(len(result.data)):\n",
    "                for j in range(len(result.data[0])):\n",
    "                    for k in range(len(result.data[0])):\n",
    "                        grad_val = result.data[i][j] * ((1 if j == k else 0) - result.data[i][k])\n",
    "                        x.grad[i][k] += result.grad[i][j] * grad_val\n",
    "        \n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    \"tanh\": ActivationFunctions.tanh,\n",
    "    \"relu\": ActivationFunctions.relu,\n",
    "    \"sigmoid\": ActivationFunctions.sigmoid,\n",
    "    \"softmax\": ActivationFunctions.softmax,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164d7481-5b33-453d-8571-fbb731cb6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    @staticmethod\n",
    "    def mse(output, target):\n",
    "        squared_diffs = [[(o - t) ** 2 for o, t in zip(out_row, target_row)] \n",
    "                          for out_row, target_row in zip(output.data, target.data)]\n",
    "        \n",
    "        loss_value = sum(sum(row) for row in squared_diffs) / (len(output.data) * len(output.data[0]))\n",
    "\n",
    "        loss_tensor = Tensor([[loss_value]])\n",
    "        loss_tensor._prev = [output] \n",
    "\n",
    "        def _backward():\n",
    "            if output.grad is None:\n",
    "                output.grad = [[0.0 for _ in range(len(output.data[0]))] for _ in range(len(output.data))]\n",
    "            for i in range(len(output.data)):\n",
    "                for j in range(len(output.data[0])):\n",
    "                    output.grad[i][j] += (2 * (output.data[i][j] - target.data[i][j])) / (len(output.data) * len(output.data[0]))\n",
    "\n",
    "        loss_tensor._backward = _backward  \n",
    "\n",
    "        return loss_tensor\n",
    "\n",
    "err_functions = {\n",
    "    \"mse\": ErrorFunction.mse,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79c48a1-1968-42bf-bf04-149fd156a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        self.set_data(data)\n",
    "        self.grad = None\n",
    "        self._backward = lambda: None\n",
    "        self._prev = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, prev={self._prev} shape={self.shape})\"\n",
    "\n",
    "    def set_data(self,data):\n",
    "        self.data = data\n",
    "        self.determine_shape()\n",
    "\n",
    "    def determine_shape(self):\n",
    "        data = self.data\n",
    "        if isinstance(data[0], list):\n",
    "            rows = len(data)\n",
    "            cols = len(data[0]) if rows > 0 else 0\n",
    "            self.shape = (rows, cols)\n",
    "        else:\n",
    "            self.shape = (len(data), 1)\n",
    "\n",
    "    def dot(self, other):\n",
    "        v1 = self.data  \n",
    "        v2 = other.data \n",
    "        \n",
    "        m = len(v1)      \n",
    "        n = len(v2[0])  \n",
    "        p = len(v2)    \n",
    "    \n",
    "        if len(v1[0]) != len(v2):\n",
    "            raise ValueError(\"Incompatible dimensions for matrix multiplication.\")\n",
    "        \n",
    "        result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    \n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                for k in range(p):\n",
    "                    result[i][j] += v1[i][k] * v2[k][j]        \n",
    "        output = Tensor(result)\n",
    "        \n",
    "        def _backward():\n",
    "            \n",
    "            if self.grad is None:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            if other.grad is None:\n",
    "                other.grad = [[0 for _ in range(len(other.data[0]))] for _ in range(len(other.data))]\n",
    "            \n",
    "            for i in range(m):\n",
    "                for k in range(p):\n",
    "                    for j in range(n):\n",
    "                        self.grad[i][k] += output.grad[i][j] * other.data[k][j]\n",
    "            \n",
    "            self_T = self.transpose()\n",
    "            for k in range(p):\n",
    "                for j in range(n):\n",
    "                    for i in range(m):\n",
    "                        other.grad[k][j] += output.grad[i][j] * self_T.data[k][i]\n",
    "        \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def broadcasted(self, m):\n",
    "        result = []\n",
    "        for _ in range(m):\n",
    "            result.append(self.data)\n",
    "        return Tensor(result)\n",
    "\n",
    "    def add(self, other):\n",
    "        compatible_other = other\n",
    "        if self.shape != other.shape or not isinstance(other.data[0], list):\n",
    "            compatible_other = other.broadcasted(self.shape[0])\n",
    "    \n",
    "        result = []\n",
    "        for i in range(len(self.data)):\n",
    "            row = []\n",
    "            for j in range(len(self.data[0])):\n",
    "                row.append(self.data[i][j] + compatible_other.data[i][j])\n",
    "            result.append(row)\n",
    "        output = Tensor(result)\n",
    "    \n",
    "        def _backward():\n",
    "            if self.grad is None:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            for i in range(len(self.data)):\n",
    "                for j in range(len(self.data[0])):\n",
    "                    self.grad[i][j] += output.grad[i][j]\n",
    "            \n",
    "            if other.grad is None:\n",
    "                other.grad = [0 for _ in range(len(other.data))]\n",
    "    \n",
    "            for i in range(len(output.data)):\n",
    "                for j in range(len(output.data[0])):\n",
    "                    other.grad[j] += output.grad[i][j]\n",
    "    \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    def step(self, lr):\n",
    "        if(isinstance(self.data[0], list)):\n",
    "            for i in range(len(self.data)):\n",
    "                for j in range(len(self.data[0])):\n",
    "                    self.data[i][j] += self.grad[i][j] * -lr\n",
    "        else:\n",
    "            for i in range(len(self.data)):\n",
    "                self.data[i] += self.grad[i] * -lr\n",
    "\n",
    "    def transpose(self):\n",
    "        transposed_data = [[self.data[j][i] for j in range(self.shape[0])]\n",
    "                           for i in range(self.shape[1])]\n",
    "        return Tensor(transposed_data)\n",
    "\n",
    "    def backward(self):\n",
    "        if self.grad is None:\n",
    "            self.grad = [[1.0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "        self._backward()\n",
    "        for prev_tensor in self._prev:\n",
    "            prev_tensor.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766ff2ec-899b-4222-8982-78da68868609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self,nin,nout,act_func = \"tanh\"):\n",
    "        self.weights = Tensor([[i + 1 for i in range(nout)] for _ in range(nin)])\n",
    "        self.biases = Tensor([1 for _ in range(nout)])\n",
    "        \n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = Tensor(weights)\n",
    "        self.biases = Tensor(biases)\n",
    "        \n",
    "    def set_biases(self, biases):\n",
    "        self.biases = Tensor(biases)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        result = inputs.dot(self.weights).add(self.biases)\n",
    "        return result\n",
    "        \n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activations = []):\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(DenseLayer(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if(i < len(activations)):\n",
    "                self.activations.append(None if activations[i] == None else activations[i])\n",
    "            else:\n",
    "                self.activations.append(None)\n",
    "        \n",
    "            \n",
    "    def set_parameters(self,weights, biases):\n",
    "        for layer,weight, bias in zip(self.layers,weights, biases):\n",
    "            layer.set_parameters(weight, bias)\n",
    "            \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            inputs = layer.forward(inputs)\n",
    "            if self.activations[idx]:\n",
    "                inputs = activation_functions[self.activations[idx]](inputs)\n",
    "            \n",
    "        return inputs\n",
    "\n",
    "    def get_parameters(self):\n",
    "        all_weights = []\n",
    "        all_biases = []\n",
    "        for layer in self.layers:\n",
    "            weights, biases = layer.get_parameters()\n",
    "            all_weights.append(weights)\n",
    "            all_biases.append(biases)\n",
    "            \n",
    "        return all_weights, all_biases\n",
    "\n",
    "    def step(self, lr):\n",
    "        weights, biases = self.get_parameters()\n",
    "        for param in weights + biases:\n",
    "            param.step(lr)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        weights, biases = self.get_parameters()\n",
    "        for param in weights + biases:\n",
    "            param.grad = None\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621105e-4774-424c-a33e-d571cb58ddbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
