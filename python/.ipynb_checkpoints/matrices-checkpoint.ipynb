{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "675f2dfd-e596-4340-bb40-fc71d85542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            tanh_row = [math.tanh(_x) for _x in row]\n",
    "            result.append(tanh_row)\n",
    "        result = Tensor(result)\n",
    "        result._prev = [x]\n",
    "        return result\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    \"tanh\": ActivationFunctions.tanh,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164d7481-5b33-453d-8571-fbb731cb6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    @staticmethod\n",
    "    def mse(output, target):\n",
    "        if output.shape != target.shape or not isinstance(target.data[0], list):\n",
    "            target.broadcast(output.shape[0])\n",
    "        error_sum = 0.0\n",
    "        for i in range(len(output.data)):\n",
    "            for j in range(len(output.data[0])):\n",
    "                error_sum += (output.data[i][j] - target.data[i][j]) ** 2\n",
    "        \n",
    "        return error_sum / (output.shape[0] * output.shape[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_prime(output, target):\n",
    "        \"\"\"Calculates the gradient of MSE with respect to the output.\"\"\"\n",
    "        return [(o - t) for o, t in zip(output, target)]\n",
    "\n",
    "err_functions = {\n",
    "    \"mse\": ErrorFunction.mse,\n",
    "    \"mse_prime\": ErrorFunction.mse_prime\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79c48a1-1968-42bf-bf04-149fd156a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        self.set_data(data)\n",
    "        self.grad = None\n",
    "        self._backward = lambda: None\n",
    "        self._prev = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, prev={self._prev} shape={self.shape})\"\n",
    "\n",
    "    def set_data(self,data):\n",
    "        self.data = data\n",
    "        self.determine_shape()\n",
    "\n",
    "    def determine_shape(self):\n",
    "        data = self.data\n",
    "        if isinstance(data[0], list):\n",
    "            rows = len(data)\n",
    "            cols = len(data[0]) if rows > 0 else 0\n",
    "            self.shape = (rows, cols)\n",
    "        else:\n",
    "            self.shape = (len(data), 1)\n",
    "\n",
    "    def dot(self, other):\n",
    "        v1 = self.data  \n",
    "        v2 = other.data \n",
    "        \n",
    "        m = len(v1)      \n",
    "        n = len(v2[0])  \n",
    "        p = len(v2)    \n",
    "    \n",
    "        if len(v1[0]) != len(v2):\n",
    "            raise ValueError(\"Incompatible dimensions for matrix multiplication.\")\n",
    "        \n",
    "        result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    \n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                for k in range(p):\n",
    "                    result[i][j] += v1[i][k] * v2[k][j]        \n",
    "        output = Tensor(result)\n",
    "        \n",
    "        def _backward():\n",
    "            \n",
    "            # Grad w.r.t. input (self)\n",
    "            if self.grad is None:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            if other.grad is None:\n",
    "                other.grad = [[0 for _ in range(len(other.data[0]))] for _ in range(len(other.data))]\n",
    "            \n",
    "            # Grad w.r.t. self (inputs)\n",
    "            for i in range(m):\n",
    "                for k in range(p):\n",
    "                    for j in range(n):\n",
    "                        self.grad[i][k] += output.grad[i][j] * other.data[k][j]\n",
    "            \n",
    "            # Grad w.r.t. other (weights)\n",
    "            self_T = self.transpose()\n",
    "            for k in range(p):\n",
    "                for j in range(n):\n",
    "                    for i in range(m):\n",
    "                        other.grad[k][j] += output.grad[i][j] * self_T.data[k][i]\n",
    "        \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def broadcast(self, m):\n",
    "        result = []\n",
    "        for _ in range(m):\n",
    "            result.append(self.data)\n",
    "        self.set_data(result)\n",
    "\n",
    "    def add(self, other):\n",
    "        if self.shape != other.shape or not isinstance(other.data[0], list):\n",
    "            other.broadcast(self.shape[0])\n",
    "        \n",
    "        result = []\n",
    "        for i in range(len(self.data)):\n",
    "            row = []\n",
    "            for j in range(len(self.data[0])):\n",
    "                row.append(self.data[i][j] + other.data[i][j])\n",
    "            result.append(row)\n",
    "        output = Tensor(result)\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient w.r.t. self (the first term in the addition)\n",
    "            if self.grad is None:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            for i in range(len(self.data)):\n",
    "                for j in range(len(self.data[0])):\n",
    "                    self.grad[i][j] += output.grad[i][j]\n",
    "    \n",
    "            # Gradient w.r.t. other (the second term in the addition)\n",
    "            if other.grad is None:\n",
    "                other.grad = [[0 for _ in range(len(other.data[0]))] for _ in range(len(other.data))]\n",
    "            for i in range(len(other.data)):\n",
    "                for j in range(len(other.data[0])):\n",
    "                    other.grad[i][j] += output.grad[i][j]\n",
    "    \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "        \n",
    "\n",
    "    def transpose(self):\n",
    "        transposed_data = [[self.data[j][i] for j in range(self.shape[0])]\n",
    "                           for i in range(self.shape[1])]\n",
    "        return Tensor(transposed_data)\n",
    "\n",
    "    def backward(self):\n",
    "        if self.grad is None:\n",
    "            self.grad = [[1.0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "        self._backward()\n",
    "        for prev_tensor in self._prev:\n",
    "            prev_tensor.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "766ff2ec-899b-4222-8982-78da68868609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self,nin,nout,act_func = \"tanh\"):\n",
    "        self.weights = Tensor([[i + 1 for i in range(nout)] for _ in range(nin)])\n",
    "        self.biases = Tensor([1 for _ in range(nout)])\n",
    "        self.act_func = None if act_func == \"\" else activation_functions[act_func]\n",
    "        \n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = Tensor(weights)\n",
    "        self.biases = Tensor(biases)\n",
    "        \n",
    "    def set_biases(self, biases):\n",
    "        self.biases = Tensor(biases)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs.dot(self.weights).add(self.biases)\n",
    "        \n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, err_func=\"mse\"):\n",
    "        self.layers = []\n",
    "        self.err_func = err_functions[err_func]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = DenseLayer(layer_sizes[i], layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    def set_parameters(self,weights, biases):\n",
    "        for layer,weight, bias in zip(self.layers,weights, biases):\n",
    "            layer.set_parameters(weight, bias)\n",
    "            \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def get_parameters(self):\n",
    "        all_weights = []\n",
    "        all_biases = []\n",
    "        for layer in self.layers:\n",
    "            weights, biases = layer.get_parameters()\n",
    "            all_weights.append(weights)\n",
    "            all_biases.append(biases)\n",
    "            \n",
    "        return all_weights, all_biases\n",
    "            \n",
    "    \n",
    "    def train(self,x,y, batch_size):\n",
    "        num_samples = len(x)\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            x_batch = Tensor(x[i:i + batch_size])\n",
    "            y_batch = Tensor(y[i:i + batch_size])\n",
    "            \n",
    "            output = self.forward(x_batch)\n",
    "            layer = self.layers[0]\n",
    "          \n",
    "            output.backward()\n",
    "            return output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "651420b6-bb03-4e51-aaa6-a09f8deca542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result for weights of layer 1: True\n",
      "\n",
      "Comparison result for weights of layer 2: True\n",
      "\n",
      "Comparison result for biases of layer 1: True\n",
      "\n",
      "Comparison result for biases of layer 2: True\n",
      "\n",
      "mlp [[0.03301823812879712]]\n",
      "***********\n",
      "torch tensor([[0.0330]])\n",
      "mlp [[-0.6542797201119002]]\n",
      "***********\n",
      "torch tensor([[-0.6543]])\n",
      "mlp [[0.8726628909616414]]\n",
      "***********\n",
      "torch tensor([[0.8727]])\n",
      "mlp [[-0.10182294515073698]]\n",
      "***********\n",
      "torch tensor([[-0.1018]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class MLP_TORCH(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(MLP_TORCH, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "        self.network = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.network:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def set_parameters(self, weights, biases):\n",
    "        with torch.no_grad():\n",
    "            for idx, layer in enumerate(self.network):\n",
    "                layer.weight = nn.Parameter(torch.tensor(weights[idx], dtype=torch.float32).T)\n",
    "                layer.bias = nn.Parameter(torch.tensor(biases[idx], dtype=torch.float32))\n",
    "                \n",
    "    def get_parameters(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            weights.append(layer.weight)\n",
    "            biases.append(layer.bias)\n",
    "        return weights, biases\n",
    "\n",
    "def compare_weights_and_biases_values(mlp_torch, mlp):\n",
    "    expected_weights, expected_biases = mlp_torch.get_parameters()\n",
    "    weights, biases = mlp.get_parameters()\n",
    "    weights = [torch.tensor(weight.data) for weight in weights]\n",
    "    biases = [torch.tensor(bias.data) for bias in biases]\n",
    "    \n",
    "    for i, (expected_w, w) in enumerate(zip(expected_weights, weights)):\n",
    "        comparison_result = torch.allclose(expected_w.T, w, rtol=1e-04, atol=1e-04)\n",
    "        print(f\"Comparison result for weights of layer {i+1}: {comparison_result}\")\n",
    "        if not comparison_result:\n",
    "            print(f\"Difference in weights for layer {i+1}:\")\n",
    "            print(f\"Expected: {expected_w}\")\n",
    "            print(f\"Provided: {w}\")\n",
    "            assert False\n",
    "        print()\n",
    "    \n",
    "    for i, (expected_b, b) in enumerate(zip(expected_biases, biases)):\n",
    "        comparison_result = torch.allclose(expected_b.flatten(), b.flatten(), rtol=1e-04, atol=1e-04)\n",
    "        print(f\"Comparison result for biases of layer {i+1}: {comparison_result}\")\n",
    "        if not comparison_result:\n",
    "            print(f\"Difference in biases for layer {i+1}:\")\n",
    "            print(f\"Expected: {expected_b}\")\n",
    "            print(f\"Provided: {b.flatten()}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def compare_mlp(layer_sizes, x, y, batch_size=1):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(len(layer_sizes) - 1):\n",
    "        weights.append([[random.uniform(-1, 1) for _ in range(layer_sizes[l + 1])] for _ in range(layer_sizes[l])])\n",
    "        biases.append([random.uniform(-1, 1) for _ in range(layer_sizes[l + 1])])\n",
    "    mlp = MLP(layer_sizes)\n",
    "    mlp.set_parameters(weights, biases)\n",
    "\n",
    "    mlp_torch = MLP_TORCH(layer_sizes)\n",
    "    mlp_torch.set_parameters(weights, biases)\n",
    "\n",
    "    mlp_torch.get_parameters()\n",
    "    mlp.get_parameters()\n",
    "    compare_weights_and_biases_values(mlp_torch, mlp)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        x_batch = x[i:i + batch_size]\n",
    "        y_batch = y[i:i + batch_size]\n",
    "        print(\"mlp\",mlp.forward(Tensor(x_batch)).data)\n",
    "        print(\"***********\")\n",
    "        print(\"torch\",mlp_torch.forward(torch.tensor(x_batch)).data)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "compare_mlp([3,3,1],[\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "],[1.0, -1.0, -1.0, 1.0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0a03b68-89af-417c-babd-8e77941b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compare_with_torch(inputs, weights, biases,act_func):\n",
    "    layer = DenseLayer(len(inputs),len(biases),act_func)\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "    layer.set_biases(biases)\n",
    "    output = layer.forward(inputs)\n",
    "\n",
    "    x = torch.tensor(inputs, dtype=torch.double, requires_grad=True)\n",
    "    w = torch.tensor(weights, dtype=torch.double, requires_grad=True)\n",
    "    b = torch.tensor(biases, dtype=torch.double, requires_grad=True)\n",
    "    print(x)\n",
    "\n",
    "    n = torch.matmul(w.T, x) + b\n",
    "\n",
    "    expected_output = getattr(torch, act_func)(n)\n",
    "    \n",
    "    assert torch.allclose(expected_output, torch.tensor([output], dtype=torch.float64), atol=1e-6)\n",
    "\n",
    "    expected_output.sum().backward()\n",
    "    expected_w_grad = w.grad.detach()\n",
    "    expected_b_grad = b.grad.detach()\n",
    "    expected_x_grad = x.grad.detach()\n",
    "\n",
    "    (w_grad,b_grad,x_grad) = layer.backward()\n",
    "    assert torch.allclose(expected_w_grad, torch.tensor([w_grad], dtype=torch.float64), atol=1e-6)\n",
    "    assert torch.allclose(expected_b_grad, torch.tensor([b_grad], dtype=torch.float64), atol=1e-6)\n",
    "    assert torch.allclose(expected_x_grad, torch.tensor([x_grad], dtype=torch.float64), atol=1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b8b369fe-a749-40bb-b0cd-7a72382abcc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m test_cases \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     ([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m], [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m1\u001b[39m]], [\u001b[38;5;241m6.8813735870195432\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     ([\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m], [[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m3\u001b[39m]], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, weights, biases, act_func \u001b[38;5;129;01min\u001b[39;00m test_cases:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mcompare_with_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[251], line 7\u001b[0m, in \u001b[0;36mcompare_with_torch\u001b[0;34m(inputs, weights, biases, act_func)\u001b[0m\n\u001b[1;32m      5\u001b[0m layer\u001b[38;5;241m.\u001b[39mset_weights(weights)\n\u001b[1;32m      6\u001b[0m layer\u001b[38;5;241m.\u001b[39mset_biases(biases)\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[239], line 20\u001b[0m, in \u001b[0;36mDenseLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    ([2, 0], [[-3], [1]], [6.8813735870195432], \"tanh\"),\n",
    "    ([3, 2], [[2, -1], [0.5, 3]], [1, -2], \"relu\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"tanh\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"relu\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"sigmoid\"),\n",
    "    \n",
    "]\n",
    "\n",
    "for inputs, weights, biases, act_func in test_cases:\n",
    "    compare_with_torch(inputs, weights, biases, act_func)\n",
    "    print(\"Passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db938989-d4bd-4472-8c7e-4485fe180c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b57a7-6743-4805-a4de-c73b11f97f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
