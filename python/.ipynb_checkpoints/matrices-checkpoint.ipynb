{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "675f2dfd-e596-4340-bb40-fc71d85542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            tanh_row = [math.tanh(_x) for _x in row]\n",
    "            result.append(tanh_row)\n",
    "        return Tensor(result)\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    \"tanh\": ActivationFunctions.tanh,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "164d7481-5b33-453d-8571-fbb731cb6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    @staticmethod\n",
    "    def mse(output, target):\n",
    "        if output.shape != target.shape or not isinstance(target.data[0], list):\n",
    "            target = target.broadcast(output.shape[0])\n",
    "        print(output,target)\n",
    "        error_sum = 0.0\n",
    "        for i in range(len(output.data)):\n",
    "            for j in range(len(output.data[0])):\n",
    "                error_sum += (output.data[i][j] - target.data[i][j]) ** 2\n",
    "        \n",
    "        return error_sum / (output.shape[0] * output.shape[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_prime(output, target):\n",
    "        \"\"\"Calculates the gradient of MSE with respect to the output.\"\"\"\n",
    "        return [(o - t) for o, t in zip(output, target)]\n",
    "\n",
    "err_functions = {\n",
    "    \"mse\": ErrorFunction.mse,\n",
    "    \"mse_prime\": ErrorFunction.mse_prime\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f79c48a1-1968-42bf-bf04-149fd156a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        if isinstance(data[0], list):\n",
    "            rows = len(data)\n",
    "            cols = len(data[0]) if rows > 0 else 0\n",
    "            self.shape = (rows, cols)\n",
    "        else:\n",
    "            self.shape = (len(data), 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, shape={self.shape})\"\n",
    "\n",
    "    def dot(self, other):\n",
    "        v1 = self.data  \n",
    "        v2 = other.data \n",
    "        \n",
    "        m = len(v1)      \n",
    "        n = len(v2[0])  \n",
    "        p = len(v2)    \n",
    "    \n",
    "        if len(v1[0]) != len(v2):\n",
    "            raise ValueError(\"Incompatible dimensions for matrix multiplication.\")\n",
    "        \n",
    "        result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    \n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                for k in range(p):\n",
    "                    result[i][j] += v1[i][k] * v2[k][j]        \n",
    "        output = Tensor(result)\n",
    "        \n",
    "        def _backward():\n",
    "            if True or self.requires_grad:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "                other_T = other.transpose()\n",
    "                for i in range(m):\n",
    "                    for k in range(p):\n",
    "                        for j in range(n):\n",
    "                            self.grad[i][k] += out.grad[i][j] * other_T.data[j][k]\n",
    "            \n",
    "            if True or other.requires_grad:\n",
    "                other.grad = [[0 for _ in range(len(other.data[0]))] for _ in range(len(other.data))]\n",
    "                self_T = self.transpose()\n",
    "                for k in range(p):\n",
    "                    for i in range(m):\n",
    "                        for j in range(n):\n",
    "                            other.grad[k][j] += output.grad[i][j] * self_T.data[k][i]\n",
    "            \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def broadcast(self, m):\n",
    "        result = []\n",
    "        for _ in range(m):\n",
    "            result.append(self.data)\n",
    "        return Tensor(result)\n",
    "\n",
    "    def add(self, other):\n",
    "        if self.shape != other.shape or not isinstance(other.data[0], list):\n",
    "            other = other.broadcast(self.shape[0])\n",
    "        \n",
    "        result = []\n",
    "        for i in range(len(self.data)):\n",
    "            row = []\n",
    "            for j in range(len(self.data[0])):\n",
    "                row.append(self.data[i][j] + other.data[i][j])\n",
    "            result.append(row)\n",
    "        output = Tensor(result)\n",
    "        def _backward():\n",
    "            if True or self.requires_grad:\n",
    "                self.grad = [[self.grad[i][j] + out.grad[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data))]\n",
    "            if True or other.requires_grad:\n",
    "                other.grad = [[other.grad[i][j] + out.grad[i][j] for j in range(len(other.data[0]))] for i in range(len(other.data))]\n",
    "\n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def transpose(self):\n",
    "        transposed_data = [[self.data[j][i] for j in range(self.shape[0])]\n",
    "                           for i in range(self.shape[1])]\n",
    "        return Tensor(transposed_data)\n",
    "    \n",
    "    def backward(self):\n",
    "        if self.grad is None:\n",
    "            self.grad = [[1.0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            self._backward()\n",
    "        for prev_tensor in self._prev:\n",
    "        prev_tensor.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "766ff2ec-899b-4222-8982-78da68868609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[0.9950538575111004]], shape=(1, 1)) Tensor(data=[[1.0]], shape=(1, 1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4464325520497674e-05"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self,nin,nout,act_func = \"tanh\"):\n",
    "        self.weights = Tensor([[i + 1 for i in range(nout)] for _ in range(nin)])\n",
    "        self.biases = Tensor([1 for _ in range(nout)])\n",
    "        self.act_func = None if act_func == \"\" else activation_functions[act_func]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.act_func(inputs.dot(self.weights).add(self.biases))\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, err_func=\"mse\"):\n",
    "        self.layers = []\n",
    "        self.err_func = err_functions[err_func]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = DenseLayer(layer_sizes[i], layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    def train(self,x,y, batch_size):\n",
    "        num_samples = len(x)\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            x_batch = Tensor(x[i:i + batch_size])\n",
    "            y_batch = Tensor(y[i:i + batch_size])\n",
    "            \n",
    "            output = self.forward(x_batch)\n",
    "            loss = self.err_func(output, y_batch)\n",
    "            \n",
    "            return loss\n",
    "            \n",
    "            d_output = self.loss_derivative(output, y_batch)\n",
    "            self.backward(d_output, learning_rate)\n",
    "        \n",
    "\n",
    "\n",
    "layer_sizes = [3,2,1]  \n",
    "\n",
    "mlp = MLP(layer_sizes)\n",
    "\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "mlp.train(xs,ys,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d0a03b68-89af-417c-babd-8e77941b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compare_with_torch(inputs, weights, biases,act_func):\n",
    "    layer = DenseLayer(len(inputs),len(biases),act_func)\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "    layer.set_biases(biases)\n",
    "    output = layer.forward(inputs)\n",
    "\n",
    "    x = torch.tensor(inputs, dtype=torch.double, requires_grad=True)\n",
    "    w = torch.tensor(weights, dtype=torch.double, requires_grad=True)\n",
    "    b = torch.tensor(biases, dtype=torch.double, requires_grad=True)\n",
    "    print(x)\n",
    "\n",
    "    n = torch.matmul(w.T, x) + b\n",
    "\n",
    "    expected_output = getattr(torch, act_func)(n)\n",
    "    \n",
    "    assert torch.allclose(expected_output, torch.tensor([output], dtype=torch.float64), atol=1e-6)\n",
    "\n",
    "    expected_output.sum().backward()\n",
    "    expected_w_grad = w.grad.detach()\n",
    "    expected_b_grad = b.grad.detach()\n",
    "    expected_x_grad = x.grad.detach()\n",
    "\n",
    "    (w_grad,b_grad,x_grad) = layer.backward()\n",
    "    assert torch.allclose(expected_w_grad, torch.tensor([w_grad], dtype=torch.float64), atol=1e-6)\n",
    "    assert torch.allclose(expected_b_grad, torch.tensor([b_grad], dtype=torch.float64), atol=1e-6)\n",
    "    assert torch.allclose(expected_x_grad, torch.tensor([x_grad], dtype=torch.float64), atol=1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8b369fe-a749-40bb-b0cd-7a72382abcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 0.], dtype=torch.float64, requires_grad=True)\n",
      "Passed!\n",
      "tensor([3., 2.], dtype=torch.float64, requires_grad=True)\n",
      "Passed!\n",
      "tensor([3., 2., 5., 7.], dtype=torch.float64, requires_grad=True)\n",
      "Passed!\n",
      "tensor([3., 2., 5., 7.], dtype=torch.float64, requires_grad=True)\n",
      "Passed!\n",
      "tensor([3., 2., 5., 7.], dtype=torch.float64, requires_grad=True)\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    ([2, 0], [[-3], [1]], [6.8813735870195432], \"tanh\"),\n",
    "    ([3, 2], [[2, -1], [0.5, 3]], [1, -2], \"relu\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"tanh\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"relu\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"sigmoid\"),\n",
    "    \n",
    "]\n",
    "\n",
    "for inputs, weights, biases, act_func in test_cases:\n",
    "    compare_with_torch(inputs, weights, biases, act_func)\n",
    "    print(\"Passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db938989-d4bd-4472-8c7e-4485fe180c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b57a7-6743-4805-a4de-c73b11f97f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
