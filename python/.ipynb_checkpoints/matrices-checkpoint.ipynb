{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "675f2dfd-e596-4340-bb40-fc71d85542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        result = []\n",
    "        for row in x.data:\n",
    "            tanh_row = [math.tanh(_x) for _x in row]\n",
    "            result.append(tanh_row)\n",
    "        result = Tensor(result)\n",
    "        result._prev = [x]\n",
    "        return result\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    \"tanh\": ActivationFunctions.tanh,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "164d7481-5b33-453d-8571-fbb731cb6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    @staticmethod\n",
    "    def mse(output, target):\n",
    "        if output.shape != target.shape or not isinstance(target.data[0], list):\n",
    "            target.broadcast(output.shape[0])\n",
    "        error_sum = 0.0\n",
    "        for i in range(len(output.data)):\n",
    "            for j in range(len(output.data[0])):\n",
    "                error_sum += (output.data[i][j] - target.data[i][j]) ** 2\n",
    "        \n",
    "        return error_sum / (output.shape[0] * output.shape[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_prime(output, target):\n",
    "        \"\"\"Calculates the gradient of MSE with respect to the output.\"\"\"\n",
    "        return [(o - t) for o, t in zip(output, target)]\n",
    "\n",
    "err_functions = {\n",
    "    \"mse\": ErrorFunction.mse,\n",
    "    \"mse_prime\": ErrorFunction.mse_prime\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f79c48a1-1968-42bf-bf04-149fd156a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        self.set_data(data)\n",
    "        self.grad = None\n",
    "        self._backward = lambda: None\n",
    "        self._prev = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, prev={self._prev} shape={self.shape})\"\n",
    "\n",
    "    def set_data(self,data):\n",
    "        self.data = data\n",
    "        self.determine_shape()\n",
    "\n",
    "    def determine_shape(self):\n",
    "        data = self.data\n",
    "        if isinstance(data[0], list):\n",
    "            rows = len(data)\n",
    "            cols = len(data[0]) if rows > 0 else 0\n",
    "            self.shape = (rows, cols)\n",
    "        else:\n",
    "            self.shape = (len(data), 1)\n",
    "\n",
    "    def dot(self, other):\n",
    "        v1 = self.data  \n",
    "        v2 = other.data \n",
    "        \n",
    "        m = len(v1)      \n",
    "        n = len(v2[0])  \n",
    "        p = len(v2)    \n",
    "    \n",
    "        if len(v1[0]) != len(v2):\n",
    "            raise ValueError(\"Incompatible dimensions for matrix multiplication.\")\n",
    "        \n",
    "        result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    \n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                for k in range(p):\n",
    "                    result[i][j] += v1[i][k] * v2[k][j]        \n",
    "        output = Tensor(result)\n",
    "        \n",
    "        def _backward():\n",
    "            \n",
    "            # Grad w.r.t. input (self)\n",
    "            if self.grad is None:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            if other.grad is None:\n",
    "                other.grad = [[0 for _ in range(len(other.data[0]))] for _ in range(len(other.data))]\n",
    "            \n",
    "            # Grad w.r.t. self (inputs)\n",
    "            for i in range(m):\n",
    "                for k in range(p):\n",
    "                    for j in range(n):\n",
    "                        self.grad[i][k] += output.grad[i][j] * other.data[k][j]\n",
    "            \n",
    "            # Grad w.r.t. other (weights)\n",
    "            self_T = self.transpose()\n",
    "            for k in range(p):\n",
    "                for j in range(n):\n",
    "                    for i in range(m):\n",
    "                        other.grad[k][j] += output.grad[i][j] * self_T.data[k][i]\n",
    "        \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def broadcast(self, m):\n",
    "        result = []\n",
    "        for _ in range(m):\n",
    "            result.append(self.data)\n",
    "        self.set_data(result)\n",
    "\n",
    "    def add(self, other):\n",
    "        if self.shape != other.shape or not isinstance(other.data[0], list):\n",
    "            other.broadcast(self.shape[0])\n",
    "        \n",
    "        result = []\n",
    "        for i in range(len(self.data)):\n",
    "            row = []\n",
    "            for j in range(len(self.data[0])):\n",
    "                row.append(self.data[i][j] + other.data[i][j])\n",
    "            result.append(row)\n",
    "        output = Tensor(result)\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient w.r.t. self (the first term in the addition)\n",
    "            if self.grad is None:\n",
    "                self.grad = [[0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "            for i in range(len(self.data)):\n",
    "                for j in range(len(self.data[0])):\n",
    "                    self.grad[i][j] += output.grad[i][j]\n",
    "    \n",
    "            # Gradient w.r.t. other (the second term in the addition)\n",
    "            if other.grad is None:\n",
    "                other.grad = [[0 for _ in range(len(other.data[0]))] for _ in range(len(other.data))]\n",
    "            for i in range(len(other.data)):\n",
    "                for j in range(len(other.data[0])):\n",
    "                    other.grad[i][j] += output.grad[i][j]\n",
    "    \n",
    "        output._backward = _backward\n",
    "        output._prev = [self, other]\n",
    "        \n",
    "        return output\n",
    "        \n",
    "\n",
    "    def transpose(self):\n",
    "        transposed_data = [[self.data[j][i] for j in range(self.shape[0])]\n",
    "                           for i in range(self.shape[1])]\n",
    "        return Tensor(transposed_data)\n",
    "\n",
    "    def backward(self):\n",
    "        if self.grad is None:\n",
    "            self.grad = [[1.0 for _ in range(len(self.data[0]))] for _ in range(len(self.data))]\n",
    "        self._backward()\n",
    "        for prev_tensor in self._prev:\n",
    "            prev_tensor.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "766ff2ec-899b-4222-8982-78da68868609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Tensor(data=[[1, 2, 3], [1, 2, 3], [1, 2, 3]], grad=[[2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [-1.0, -1.0, -1.0]], prev=[] shape=(3, 3)), Tensor(data=[[1, 1, 1]], grad=[[1.0, 1.0, 1.0]], prev=[] shape=(1, 3))], [Tensor(data=[[1], [1], [1]], grad=[[5.0], [9.0], [13.0]], prev=[] shape=(3, 1)), Tensor(data=[[1]], grad=[[1.0]], prev=[] shape=(1, 1))]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self,nin,nout,act_func = \"tanh\"):\n",
    "        self.weights = Tensor([[i + 1 for i in range(nout)] for _ in range(nin)])\n",
    "        self.biases = Tensor([1 for _ in range(nout)])\n",
    "        self.act_func = None if act_func == \"\" else activation_functions[act_func]\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = Tensor(weights)\n",
    "        \n",
    "    def set_biases(self, biases):\n",
    "        self.biases = Tensor(biases)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs.dot(self.weights).add(self.biases)\n",
    "        \n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, err_func=\"mse\"):\n",
    "        self.layers = []\n",
    "        self.err_func = err_functions[err_func]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = DenseLayer(layer_sizes[i], layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return [layer.get_parameters() for layer in self.layers]\n",
    "            \n",
    "    \n",
    "    def train(self,x,y, batch_size):\n",
    "        num_samples = len(x)\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            x_batch = Tensor(x[i:i + batch_size])\n",
    "            y_batch = Tensor(y[i:i + batch_size])\n",
    "            \n",
    "            output = self.forward(x_batch)\n",
    "            layer = self.layers[0]\n",
    "          \n",
    "            output.backward()\n",
    "            print(self.get_parameters())\n",
    "            #print(\"OUTPUT\", output)\n",
    "            return \n",
    "\n",
    "\n",
    "layer_sizes = [3,3,1]  \n",
    "\n",
    "mlp = MLP(layer_sizes)\n",
    "\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "mlp.train(xs,ys,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d0a03b68-89af-417c-babd-8e77941b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compare_with_torch(inputs, weights, biases,act_func):\n",
    "    layer = DenseLayer(len(inputs),len(biases),act_func)\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "    layer.set_biases(biases)\n",
    "    output = layer.forward(inputs)\n",
    "\n",
    "    x = torch.tensor(inputs, dtype=torch.double, requires_grad=True)\n",
    "    w = torch.tensor(weights, dtype=torch.double, requires_grad=True)\n",
    "    b = torch.tensor(biases, dtype=torch.double, requires_grad=True)\n",
    "    print(x)\n",
    "\n",
    "    n = torch.matmul(w.T, x) + b\n",
    "\n",
    "    expected_output = getattr(torch, act_func)(n)\n",
    "    \n",
    "    assert torch.allclose(expected_output, torch.tensor([output], dtype=torch.float64), atol=1e-6)\n",
    "\n",
    "    expected_output.sum().backward()\n",
    "    expected_w_grad = w.grad.detach()\n",
    "    expected_b_grad = b.grad.detach()\n",
    "    expected_x_grad = x.grad.detach()\n",
    "\n",
    "    (w_grad,b_grad,x_grad) = layer.backward()\n",
    "    assert torch.allclose(expected_w_grad, torch.tensor([w_grad], dtype=torch.float64), atol=1e-6)\n",
    "    assert torch.allclose(expected_b_grad, torch.tensor([b_grad], dtype=torch.float64), atol=1e-6)\n",
    "    assert torch.allclose(expected_x_grad, torch.tensor([x_grad], dtype=torch.float64), atol=1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b8b369fe-a749-40bb-b0cd-7a72382abcc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m test_cases \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     ([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m], [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m1\u001b[39m]], [\u001b[38;5;241m6.8813735870195432\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     ([\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m], [[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m3\u001b[39m]], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, weights, biases, act_func \u001b[38;5;129;01min\u001b[39;00m test_cases:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mcompare_with_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[251], line 7\u001b[0m, in \u001b[0;36mcompare_with_torch\u001b[0;34m(inputs, weights, biases, act_func)\u001b[0m\n\u001b[1;32m      5\u001b[0m layer\u001b[38;5;241m.\u001b[39mset_weights(weights)\n\u001b[1;32m      6\u001b[0m layer\u001b[38;5;241m.\u001b[39mset_biases(biases)\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[239], line 20\u001b[0m, in \u001b[0;36mDenseLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    ([2, 0], [[-3], [1]], [6.8813735870195432], \"tanh\"),\n",
    "    ([3, 2], [[2, -1], [0.5, 3]], [1, -2], \"relu\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"tanh\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"relu\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"sigmoid\"),\n",
    "    \n",
    "]\n",
    "\n",
    "for inputs, weights, biases, act_func in test_cases:\n",
    "    compare_with_torch(inputs, weights, biases, act_func)\n",
    "    print(\"Passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db938989-d4bd-4472-8c7e-4485fe180c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b57a7-6743-4805-a4de-c73b11f97f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
