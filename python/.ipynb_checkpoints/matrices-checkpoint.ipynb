{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "675f2dfd-e596-4340-bb40-fc71d85542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        e = math.exp(2 * x)\n",
    "        output = (e - 1) / (e + 1)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_prime(x):\n",
    "        return 1 - x ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return max(x,0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_prime(x):\n",
    "        return 1 if x > 0 else 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        output = 1 / (1 + math.exp(-x))\n",
    "    \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_prime(x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        \"\"\"Applies the softmax function to each row (vector) in a matrix.\"\"\"\n",
    "        output = []\n",
    "        for row in x:\n",
    "            exp_values = [math.exp(value) for value in row]\n",
    "            sum_exp = sum(exp_values)\n",
    "            softmax_values = [value / sum_exp for value in exp_values]\n",
    "            output.append(softmax_values)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_prime(softmax_output):\n",
    "        \"\"\"Calculates the derivative of the softmax function.\"\"\"\n",
    "        # Create a Jacobian matrix\n",
    "        n = len(softmax_output)\n",
    "        jacobian = [[0] * n for _ in range(n)]\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i == j:\n",
    "                    jacobian[i][j] = softmax_output[i] * (1 - softmax_output[i])\n",
    "                else:\n",
    "                    jacobian[i][j] = -softmax_output[i] * softmax_output[j]\n",
    "\n",
    "        return jacobian\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    \"tanh\": ActivationFunctions.tanh,\n",
    "    \"tanh_prime\": ActivationFunctions.tanh_prime,\n",
    "    \"relu\": ActivationFunctions.relu,\n",
    "    \"relu_prime\": ActivationFunctions.relu_prime,\n",
    "    \"sigmoid\": ActivationFunctions.sigmoid,\n",
    "    \"sigmoid_prime\": ActivationFunctions.sigmoid_prime,\n",
    "    \"softmax\": ActivationFunctions.softmax,\n",
    "    \"softmax_prime\": ActivationFunctions.softmax_prime,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "164d7481-5b33-453d-8571-fbb731cb6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunction:\n",
    "    @staticmethod\n",
    "    def mse(output, target):\n",
    "        \"\"\"Calculates the Mean Squared Error (MSE).\"\"\"\n",
    "        return sum((o - t) ** 2 for o, t in zip(output, target)) / len(target)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_prime(output, target):\n",
    "        \"\"\"Calculates the gradient of MSE with respect to the output.\"\"\"\n",
    "        return [(o - t) for o, t in zip(output, target)]\n",
    "\n",
    "err_functions = {\n",
    "    \"mse\": ErrorFunction.mse,\n",
    "    \"mse_prime\": ErrorFunction.mse_prime\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "766ff2ec-899b-4222-8982-78da68868609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed.\n",
      "Epoch 2/10 completed.\n",
      "Epoch 3/10 completed.\n",
      "Epoch 4/10 completed.\n",
      "Epoch 5/10 completed.\n",
      "Epoch 6/10 completed.\n",
      "Epoch 7/10 completed.\n",
      "Epoch 8/10 completed.\n",
      "Epoch 9/10 completed.\n",
      "Epoch 10/10 completed.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self,nin,nout, activation_func=\"tanh\"):\n",
    "        self.weights = [[random.uniform(-1, 1) for _ in range(nout)] for _ in range(nin)]\n",
    "        self.biases = [random.uniform(-1, 1) for _ in range(nout)]\n",
    "        self.activation_func = activation_functions[activation_func]\n",
    "        self.activation_func_prime = activation_functions[activation_func + \"_prime\"]\n",
    "        \n",
    "    def set_weights(self,weights):\n",
    "        self.weights = weights\n",
    "\n",
    "    def set_biases(self,biases):\n",
    "        self.biases = biases\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Performs the forward pass\"\"\"\n",
    "        self.input = inputs\n",
    "        self.output = []\n",
    "        isSoftMax = self.activation_func == ActivationFunctions.softmax\n",
    "\n",
    "        for j in range(len(self.biases)):\n",
    "            activation = self.biases[j]\n",
    "            for i in range(len(inputs)):\n",
    "                activation += inputs[i] * self.weights[i][j]\n",
    "            if(not isSoftMax):\n",
    "                self.output.append(self.activation_func(activation))\n",
    "        if(isSoftMax):\n",
    "            self.output = self.activation_func(self.output)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        d_input = [0] * len(self.input)\n",
    "        d_weights = [[0 for _ in range(len(self.biases))] for _ in range(len(self.input))]\n",
    "        d_biases = [0] * len(self.biases)\n",
    "        \n",
    "        for j in range(len(self.biases)):\n",
    "            d_activation = self.activation_func_prime(self.output[j])\n",
    "            \n",
    "            d_biases[j] = d_activation\n",
    "            \n",
    "            for i in range(len(self.input)):\n",
    "                d_weights[i][j] = self.input[i] * d_activation \n",
    "                d_input[i] += self.weights[i][j] * d_activation\n",
    "        \n",
    "        return (d_weights, d_biases, d_input)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activation_funcs, err_func=\"mse\"):\n",
    "        self.layers = []\n",
    "        self.err_func = err_functions[err_func]\n",
    "        self.err_func_prime = err_functions[err_func+ \"_prime\"]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = DenseLayer(layer_sizes[i], layer_sizes[i + 1], activation_funcs[i])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Perform the forward pass through the network.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        \"\"\"Perform the backward pass through the network.\"\"\"\n",
    "        d_input = d_output\n",
    "        for layer in reversed(self.layers):\n",
    "            d_weights, d_biases, d_input = layer.backward()\n",
    "            # Update weights and biases using the gradients (add your optimization step here)\n",
    "            layer.weights = [[w - learning_rate * dw for w, dw in zip(weights, d_weights[i])] \n",
    "                             for i, weights in enumerate(layer.weights)]\n",
    "            layer.biases = [b - learning_rate * db for b, db in zip(layer.biases, d_biases)]\n",
    "        return d_input\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                output = self.forward(X[i])\n",
    "                d_output = self.err_func_prime(output, y[i])  # Implement your loss gradient here\n",
    "                self.backward(d_output)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "layer_sizes = [2, 3, 1]  # Input layer with 2 neurons, hidden layer with 3 neurons, output layer with 1 neuron\n",
    "activation_funcs = [\"tanh\", \"sigmoid\"]  # Activation functions for each layer\n",
    "\n",
    "mlp = MLP(layer_sizes, activation_funcs)\n",
    "\n",
    "# Sample data for training (X: input data, y: target output)\n",
    "X = [[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]]\n",
    "y = [[0.5], [0.6], [0.7]]  # Example target outputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "mlp.train(X, y, epochs, learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d0a03b68-89af-417c-babd-8e77941b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_torch(inputs, weights, biases,act_func):\n",
    "    layer = DenseLayer(len(inputs),len(biases),act_func)\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "    layer.set_biases(biases)\n",
    "    output = layer.forward(inputs)\n",
    "\n",
    "    x = torch.tensor(inputs, dtype=torch.double, requires_grad=True)\n",
    "    w = torch.tensor(weights, dtype=torch.double, requires_grad=True)\n",
    "    b = torch.tensor(biases, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "    n = torch.matmul(w.T, x) + b\n",
    "\n",
    "    expected_output = getattr(torch, act_func)(n)\n",
    "    \n",
    "    assert torch.allclose(expected_output, torch.tensor([output], dtype=torch.float64), atol=1e-6)\n",
    "\n",
    "    expected_output.sum().backward()\n",
    "    expected_w_grad = w.grad.detach()\n",
    "    expected_b_grad = b.grad.detach()\n",
    "    expected_x_grad = x.grad.detach()\n",
    "\n",
    "    (w_grad,b_grad,x_grad) = layer.backward()\n",
    "    assert torch.allclose(expected_w_grad, torch.tensor([w_grad], dtype=torch.float64), atol=1e-6)\n",
    "    assert torch.allclose(expected_b_grad, torch.tensor([b_grad], dtype=torch.float64), atol=1e-6)\n",
    "    assert torch.allclose(expected_x_grad, torch.tensor([x_grad], dtype=torch.float64), atol=1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b8b369fe-a749-40bb-b0cd-7a72382abcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n",
      "Passed!\n",
      "Passed!\n",
      "Passed!\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    ([2, 0], [[-3], [1]], [6.8813735870195432], \"tanh\"),\n",
    "    ([3, 2], [[2, -1], [0.5, 3]], [1, -2], \"relu\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"tanh\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"relu\"),\n",
    "    ([3, 2, 5, 7], [[2, -1, 0.5], [0.5, 3, 1], [0.2, 0.3, 1], [0.5, 0.3, 0.2]], [0.5, 0.4, 0.1], \"sigmoid\"),\n",
    "    \n",
    "]\n",
    "\n",
    "for inputs, weights, biases, act_func in test_cases:\n",
    "    compare_with_torch(inputs, weights, biases, act_func)\n",
    "    print(\"Passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db938989-d4bd-4472-8c7e-4485fe180c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b57a7-6743-4805-a4de-c73b11f97f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
