{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d116f72d-9005-42e1-8040-b609ac724743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz\n",
    "# !pip install torch\n",
    "# !pip install tensorflow\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea9f453-183a-41ac-8258-10244e935212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import random\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, value, children=(), op = \"\"):\n",
    "        self.data = value\n",
    "        self.children = set(children)\n",
    "        self.label = \"\"\n",
    "        self.op = op\n",
    "        self.grad = 0.0\n",
    "        self.backward = lambda: None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Value({self.data})\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value({self.data})\"\n",
    "\n",
    "    def __add__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        output = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * output.grad\n",
    "            other.grad += 1.0 * output.grad\n",
    "\n",
    "        output.backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __radd__(self, other) -> Value:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        output = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += output.grad * other.data\n",
    "            other.grad += output.grad * self.data\n",
    "\n",
    "        output.backward = _backward\n",
    "        return output\n",
    "    \n",
    "    def __rmul__(self, other) -> Value:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        other = other if isinstance(other, (int, float)) else other.data\n",
    "\n",
    "        output = Value(self.data ** other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * output.grad\n",
    "    \n",
    "        output.backward = _backward\n",
    "        return output\n",
    "    \n",
    "    def __rpow__(self, other) -> Value:\n",
    "        return self.__pow__(other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def __rtruediv__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        return other * (self ** -1)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other) -> Value:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def exp(self):\n",
    "        data = self.data\n",
    "        output = Value(math.exp(data), (self, ), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += output.data * output.grad\n",
    "            \n",
    "        output.backward = _backward\n",
    "        return output\n",
    "\n",
    "    def set_label(self, label):\n",
    "        self.label = label\n",
    "\n",
    "    def set_grad(self, grad):\n",
    "        self.grad = grad\n",
    "\n",
    "    def backprop(self, initial = True):\n",
    "        # if initial: self.set_grad(1.0)\n",
    "        # self.backward()\n",
    "        # for child in self.children:\n",
    "        #     child.backprop(False)\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "    \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "    \n",
    "        build_topo(self)\n",
    "        self.set_grad(1.0)\n",
    "        for node in reversed(topo):\n",
    "            node.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2346a6e1-71e3-4cce-bdb6-ae445d816eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class ActivationFunction(enum.Enum):\n",
    "    TANH = \"tanh\"\n",
    "    RELU = \"relu\"\n",
    "    SIGMOID = \"sigmoid\"\n",
    "\n",
    "def tanh(x: Value) -> Value:\n",
    "    e = (2*x).exp()\n",
    "    output = (e - 1) / (e + 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "def relu(x: Value) -> Value:\n",
    "    data = x.data\n",
    "    output = Value(max(0, data), (x, ), 'relu')\n",
    "    \n",
    "    def _backward():\n",
    "        x.grad += int(data > 0)\n",
    "        \n",
    "    output.backward = _backward\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x: Value) -> Value:\n",
    "    output = 1 / (1 + (-x).exp())\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    ActivationFunction.TANH: tanh,\n",
    "    ActivationFunction.RELU: relu,\n",
    "    ActivationFunction.SIGMOID: sigmoid,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1eeecdc-5a89-4149-9a30-0dd507184e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\no = relu(n)\\n\\no.backprop()\\no.grad\\nw1.grad\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0); x1.set_label(\"x1\")\n",
    "x2 = Value(0.0);x2.set_label(\"x2\")\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0);w1.set_label(\"w1\")\n",
    "w2 = Value(1.0);w2.set_label(\"w2\")\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432)\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "n.backprop()\n",
    "\n",
    "\"\"\"\n",
    "o = relu(n)\n",
    "\n",
    "o.backprop()\n",
    "o.grad\n",
    "w1.grad\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1dc80e40-0e9e-42cf-985b-b79a8820b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Value(8.0) X grad:  2.0 1.0 W grad: 3.0 0.0 2.0 0.0 -1.0 0.0 B grad: 1.0 0.0\n",
      "Y Value(4.0) X grad:  3.0 3.0 W grad: 3.0 3.0 2.0 2.0 -1.0 -1.0 B grad: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# inputs \n",
    "x1 = Value(3.0)\n",
    "x2 = Value(2.0)\n",
    "x3 = Value(-1.0)\n",
    "\n",
    "# weights\n",
    "w1 = Value(2.0)\n",
    "w2 = Value(1.0)\n",
    "w3 = Value(1.0)\n",
    "w4 = Value(2.0)\n",
    "w5 = Value(1.0)\n",
    "w6 = Value(1.0)\n",
    "\n",
    "\n",
    "# bias of the neuron\n",
    "b1 = Value(1)\n",
    "b2 = Value(-2)\n",
    "# x1*w1 + x2*w2 + b1\n",
    "x1w1 = x1*w1\n",
    "x2w3 = x2*w3\n",
    "x3w5 = x3*w5\n",
    "\n",
    "x1w1x2w3x3w5 = x1w1 + x2w3 + x3w5\n",
    "y1 = x1w1x2w3x3w5 + b1\n",
    "#y1 = tanh(y1)\n",
    "\n",
    "y1.backprop()\n",
    "print(\"Y\", y1,\"X grad: \",x1.grad, x2.grad, \"W grad:\", w1.grad, w2.grad,w3.grad,w4.grad,w5.grad,w6.grad, \"B grad:\", b1.grad,b2.grad)\n",
    "\n",
    "\n",
    "\n",
    "# 2*w3 + x2*w4 + b2\n",
    "x1w2 = x1*w2\n",
    "x2w4 = x2*w4\n",
    "x3w6 = x3*w6\n",
    "\n",
    "x1w2x2w4x3w6 = x1w2 + x2w4 + x3w6\n",
    "y2 = x1w2x2w4x3w6 + b2\n",
    "\n",
    "y2.backprop()\n",
    "print(\"Y\", y2,\"X grad: \",x1.grad, x2.grad, \"W grad:\", w1.grad, w2.grad,w3.grad,w4.grad,w5.grad,w6.grad, \"B grad:\", b1.grad,b2.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57c7e6cb-255e-4e15-9261-dc69f95b0a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(0.995052961774724)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(1) for _ in range(nin)]\n",
    "        self.b = Value(1)\n",
    "        self.act_func = act_func=ActivationFunction.TANH\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        act = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        output = activation_functions[self.act_func](act)\n",
    "        return output\n",
    "      \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def change_act_func(act_func):\n",
    "        self.act_func = act_func\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout, act_func=ActivationFunction.TANH):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        self.act_func = act_func\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    def change_act_func(act_func):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.change_act_func(self.act_func)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "x = [2.0, 3.0, -1.0]\n",
    "m = MLP(3, [2, 1])\n",
    "a = m(x)\n",
    "a.backprop()\n",
    "a\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d915f00-2cbd-4c8d-ad96-423b69b4460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd788f8a-c60d-42f2-8080-4e25df893ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(0.995052961774724)\n",
      "-3.546342625308245e-08\n",
      "-5.319513937962367e-08\n",
      "1.7731713126541224e-08\n",
      "-1.7731713126541224e-08\n",
      "-3.546342625308245e-08\n",
      "-5.319513937962367e-08\n",
      "1.7731713126541224e-08\n",
      "-1.7731713126541224e-08\n",
      "-9.764174296512848e-05\n",
      "-9.764174296512848e-05\n",
      "-9.765060922420101e-05\n"
     ]
    }
   ],
   "source": [
    "ypred = m(xs[0])\n",
    "print(ypred)\n",
    "loss = (ypred - ys[0])**2\n",
    "\n",
    "for p in m.parameters():\n",
    "    p.set_grad(0.0)\n",
    "\n",
    "loss.backprop()\n",
    "\n",
    "for p in m.parameters():\n",
    "    print(p.grad)\n",
    "    \n",
    "    # p.data += -0.1 * p.grad\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3023a38f-1b59-4848-8479-514feb2597c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(0.995052961774724), Value(0.9950186691375037), Value(0.9950186691375037), Value(0.9942916279823373)]\n",
      "0.0004960762146714265\n",
      "-9.235684536130785e-06\n",
      "0.00022575486432177658\n",
      "0.00027953930317265417\n",
      "0.0004960762146714265\n",
      "-9.235684536130785e-06\n",
      "0.00022575486432177658\n",
      "0.00027953930317265417\n",
      "0.07893733186456177\n",
      "0.07893733186456177\n",
      "0.07907714907234811\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "  \n",
    "    ypred = [m(x) for x in xs]\n",
    "    print(ypred)\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "    \n",
    "    for p in m.parameters():\n",
    "        p.set_grad(0.0)\n",
    "    \n",
    "    loss.backprop()\n",
    "    \n",
    "    for p in m.parameters():\n",
    "        print(p.grad)\n",
    "        \n",
    "        # p.data += -0.1 * p.grad\n",
    "    break\n",
    "    #print(k, loss.data)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3f0ea4-a053-458f-b9ce-f69beea0a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a8c030-339b-4b9f-9565-d9588f1ad7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 18:17:15.750703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-12 18:17:15.859656: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-12 18:17:15.888870: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-12 18:17:16.094665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-12 18:17:17.230962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8813735870195432\n",
      "---\n",
      "x2 1.0\n",
      "w2 0.0\n",
      "x1 -3.0\n",
      "w1 2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.Variable([2.0], dtype=tf.float64)\n",
    "x2 = tf.Variable([0.0], dtype=tf.float64)\n",
    "w1 = tf.Variable([-3.0], dtype=tf.float64)\n",
    "w2 = tf.Variable([1.0], dtype=tf.float64)\n",
    "b = tf.Variable([6.8813735870195432], dtype=tf.float64)\n",
    "\n",
    "n = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "print(n.numpy().item())\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    n = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "grads = tape.gradient(n, [x1, x2, w1, w2, b])\n",
    "\n",
    "print('---')\n",
    "print('x2', grads[1].numpy().item())\n",
    "print('w2', grads[3].numpy().item())\n",
    "print('x1', grads[0].numpy().item())\n",
    "print('w1', grads[2].numpy().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42de596-dee4-4857-a17a-b1b2f564874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class XORNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(in_features=2, out_features=2)  # Hidden layer\n",
    "        self.output = nn.Linear(in_features=2, out_features=1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x)) \n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = XORNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0da05341-6e50-4f33-a976-a51694009108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7440416216850281\n",
      "Epoch 1000, Loss: 0.025409312918782234\n",
      "Epoch 2000, Loss: 0.010081389918923378\n",
      "Epoch 3000, Loss: 0.006137249059975147\n",
      "Epoch 4000, Loss: 0.004370155744254589\n",
      "Epoch 5000, Loss: 0.003379428992047906\n",
      "Epoch 6000, Loss: 0.0027468805201351643\n",
      "Epoch 7000, Loss: 0.0023107770830392838\n",
      "Epoch 8000, Loss: 0.0019910151604562998\n",
      "Epoch 9000, Loss: 0.0017478432273492217\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# XOR input and output data\n",
    "inputs = torch.tensor([[0.0, 0.0],\n",
    "                       [0.0, 1.0],\n",
    "                       [1.0, 0.0],\n",
    "                       [1.0, 1.0]], dtype=torch.float32)\n",
    "targets = torch.tensor([[0.0], [1.0], [1.0], [0.0]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ae133-cb50-4294-88ba-2012397209a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
