{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d116f72d-9005-42e1-8040-b609ac724743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz\n",
    "# !pip install torch\n",
    "# !pip install tensorflow\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37d12e8-0ac5-4dd8-983a-02ae22ee6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v.children:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n.op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n.op, label = n.op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n.op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2.op)\n",
    "\n",
    "  return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fea9f453-183a-41ac-8258-10244e935212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import random\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, value, children=(), op = \"\"):\n",
    "        self.data = value\n",
    "        self.children = set(children)\n",
    "        self.label = \"\"\n",
    "        self.op = op\n",
    "        self.grad = 0.0\n",
    "        self.backward = lambda: None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Value({self.data})\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value({self.data})\"\n",
    "\n",
    "    def __add__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        output = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * output.grad\n",
    "            other.grad += 1.0 * output.grad\n",
    "\n",
    "        output.backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __radd__(self, other) -> Value:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        output = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            print(self,other,output)\n",
    "            self.grad += output.grad * other.data\n",
    "            other.grad += output.grad * self.data\n",
    "\n",
    "        output.backward = _backward\n",
    "        return output\n",
    "    \n",
    "    def __rmul__(self, other) -> Value:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        other = other if isinstance(other, (int, float)) else other.data\n",
    "\n",
    "        output = Value(self.data ** other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * output.grad\n",
    "    \n",
    "        output.backward = _backward\n",
    "        return output\n",
    "    \n",
    "    def __rpow__(self, other) -> Value:\n",
    "        return self.__pow__(other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def __rtruediv__(self, other) -> Value:\n",
    "        other = Value(other) if isinstance(other, (int, float)) else other\n",
    "        return other * (self ** -1)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other) -> Value:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def exp(self):\n",
    "        data = self.data\n",
    "        output = Value(math.exp(data), (self, ), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += output.data * output.grad\n",
    "            \n",
    "        output.backward = _backward\n",
    "        return output\n",
    "\n",
    "    def set_label(self, label):\n",
    "        self.label = label\n",
    "\n",
    "    def set_grad(self, grad):\n",
    "        self.grad = grad\n",
    "\n",
    "    def backprop(self, initial = True):\n",
    "        # if initial: self.set_grad(1.0)\n",
    "        # self.backward()\n",
    "        # for child in self.children:\n",
    "        #     child.backprop(False)\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "    \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "    \n",
    "        build_topo(self)\n",
    "        self.set_grad(1.0)\n",
    "        for node in reversed(topo):\n",
    "            node.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4bd31fc-9d68-40f8-bf5b-886cff2dcb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(-6.0)\n",
      "Value(-3.0)\n",
      "Value(2.0)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.0.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"550pt\" height=\"100pt\"\n",
       " viewBox=\"0.00 0.00 550.25 100.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 96)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-96 546.25,-96 546.25,4 -4,4\"/>\n",
       "<!-- 135685399924368 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>135685399924368</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1.5,-55.5 1.5,-91.5 196.5,-91.5 196.5,-55.5 1.5,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"16.25\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">x1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"31,-56 31,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"70.88\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"110.75,-56 110.75,-91.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"153.62\" y=\"-68.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;3.0000</text>\n",
       "</g>\n",
       "<!-- 135685578263728* -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>135685578263728*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"261\" cy=\"-45.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"261\" y=\"-40.45\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 135685399924368&#45;&gt;135685578263728* -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>135685399924368&#45;&gt;135685578263728*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.76,-56.57C206.05,-54.94 215.02,-53.37 223.14,-51.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"223.64,-55.42 232.89,-50.25 222.43,-48.52 223.64,-55.42\"/>\n",
       "</g>\n",
       "<!-- 135685578263728 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>135685578263728</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"324,-27.5 324,-63.5 542.25,-63.5 542.25,-27.5 324,-27.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"350.38\" y=\"-40.7\" font-family=\"Times,serif\" font-size=\"14.00\">x1*w1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"376.75,-28 376.75,-63.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"418.88\" y=\"-40.7\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"461,-28 461,-63.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"501.62\" y=\"-40.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.0000</text>\n",
       "</g>\n",
       "<!-- 135685578263728*&#45;&gt;135685578263728 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>135685578263728*&#45;&gt;135685578263728</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M288.21,-45.5C295.29,-45.5 303.43,-45.5 312.17,-45.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.01,-49 322.01,-45.5 312.01,-42 312.01,-49\"/>\n",
       "</g>\n",
       "<!-- 135685536710928 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>135685536710928</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-36.5 198,-36.5 198,-0.5 0,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"16.25\" y=\"-13.7\" font-family=\"Times,serif\" font-size=\"14.00\">w1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"32.5,-1 32.5,-36.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"74.62\" y=\"-13.7\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"116.75,-1 116.75,-36.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"157.38\" y=\"-13.7\" font-family=\"Times,serif\" font-size=\"14.00\">grad 2.0000</text>\n",
       "</g>\n",
       "<!-- 135685536710928&#45;&gt;135685578263728* -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>135685536710928&#45;&gt;135685578263728*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M198.14,-35.06C206.99,-36.55 215.52,-37.99 223.28,-39.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"222.47,-42.72 232.91,-40.93 223.63,-35.81 222.47,-42.72\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7b67b80dc320>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0); x1.set_label(\"x1\")\n",
    "x2 = Value(0.0);x2.set_label(\"x2\")\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0);w1.set_label(\"w1\")\n",
    "w2 = Value(1.0);w2.set_label(\"w2\")\n",
    "# bias of the neuron\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "\n",
    "x1w1.backprop()\n",
    "draw_dot(x1w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eeecdc-5a89-4149-9a30-0dd507184e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0); x1.set_label(\"x1\")\n",
    "x2 = Value(0.0);x2.set_label(\"x2\")\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0);w1.set_label(\"w1\")\n",
    "w2 = Value(1.0);w2.set_label(\"w2\")\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432)\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "o = tanh(n)\n",
    "\n",
    "o.backprop()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346a6e1-71e3-4cce-bdb6-ae445d816eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class ActivationFunction(enum.Enum):\n",
    "    TANH = \"tanh\"\n",
    "    RELU = \"relu\"\n",
    "    SIGMOID = \"sigmoid\"\n",
    "\n",
    "def tanh(x: Value) -> Value:\n",
    "    e = (2*x).exp()\n",
    "    output = (e - 1) / (e + 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "def relu(x: Value) -> Value:\n",
    "    data = x.data\n",
    "    output = Value(max(0, data), (x, ), 'relu')\n",
    "    \n",
    "    def _backward():\n",
    "        x.grad += int(data > 0)\n",
    "        \n",
    "    output.backward = _backward\n",
    "    return output\n",
    "\n",
    "def sigmoid(x: Value) -> Value:\n",
    "    output = 1 / (1 + (-x).exp())\n",
    "\n",
    "    return output\n",
    "\n",
    "def softmax(x: Value) -> Value:\n",
    "    pass\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    ActivationFunction.TANH: tanh,\n",
    "    ActivationFunction.RELU: relu,\n",
    "    ActivationFunction.SIGMOID: sigmoid,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57c7e6cb-255e-4e15-9261-dc69f95b0a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(0.35699874125919623)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "        self.act_func = act_func=ActivationFunction.TANH\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        act = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        output = activation_functions[self.act_func](act)\n",
    "        return output\n",
    "      \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def change_act_func(act_func):\n",
    "        self.act_func = act_func\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout, act_func=ActivationFunction.TANH):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        self.act_func = act_func\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    def change_act_func(act_func):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.change_act_func(self.act_func)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "x = [2.0, 3.0, -1.0]\n",
    "m = MLP(3, [4, 4, 1])\n",
    "a = m(x)\n",
    "a.backprop()\n",
    "a\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d915f00-2cbd-4c8d-ad96-423b69b4460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3023a38f-1b59-4848-8479-514feb2597c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.349384764912511 1.0\n",
      "1 6.436407661108055 1.0\n",
      "2 3.589317674506558 1.0\n",
      "3 1.6744191538930504 1.0\n",
      "4 0.514367916029848 1.0\n",
      "5 0.17318029106209704 1.0\n",
      "6 0.10824430426966929 1.0\n",
      "7 0.08200315681773374 1.0\n",
      "8 0.06580162289249214 1.0\n",
      "9 0.05480222978254908 1.0\n",
      "10 0.04686044023220884 1.0\n",
      "11 0.040866967738933745 1.0\n",
      "12 0.03618969824886378 1.0\n",
      "13 0.032442229898993646 1.0\n",
      "14 0.029375256846803064 1.0\n",
      "15 0.026820854249721544 1.0\n",
      "16 0.024661862129736523 1.0\n",
      "17 0.02281410431206408 1.0\n",
      "18 0.02121557955354173 1.0\n",
      "19 0.019819634044268647 1.0\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "  \n",
    "    ypred = [m(x) for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "    \n",
    "    for p in m.parameters():\n",
    "        p.set_grad(0.0)\n",
    "    \n",
    "    loss.backprop()\n",
    "    \n",
    "    for p in m.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "    \n",
    "    print(k, loss.data, loss.grad)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da3f0ea4-a053-458f-b9ce-f69beea0a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067435836176\n",
      "---\n",
      "x2 0.20710679676218968\n",
      "w2 0.0\n",
      "x1 -0.621320390286569\n",
      "w1 0.41421359352437936\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.sigmoid(n)\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92a8c030-339b-4b9f-9565-d9588f1ad7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 10:07:25.034112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 10:07:25.178198: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 10:07:25.179127: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 10:07:25.391271: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 10:07:26.649358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8813735870195432\n",
      "---\n",
      "x2 1.0\n",
      "w2 0.0\n",
      "x1 -3.0\n",
      "w1 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 10:07:27.943462: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.Variable([2.0], dtype=tf.float64)\n",
    "x2 = tf.Variable([0.0], dtype=tf.float64)\n",
    "w1 = tf.Variable([-3.0], dtype=tf.float64)\n",
    "w2 = tf.Variable([1.0], dtype=tf.float64)\n",
    "b = tf.Variable([6.8813735870195432], dtype=tf.float64)\n",
    "\n",
    "n = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "print(n.numpy().item())\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    n = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "grads = tape.gradient(n, [x1, x2, w1, w2, b])\n",
    "\n",
    "print('---')\n",
    "print('x2', grads[1].numpy().item())\n",
    "print('w2', grads[3].numpy().item())\n",
    "print('x1', grads[0].numpy().item())\n",
    "print('w1', grads[2].numpy().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
