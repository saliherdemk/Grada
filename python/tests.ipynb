{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada33df9-c3e3-4a90-a684-ac589b4ab7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run matrices.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77a7b3e-1f3c-42ec-a3dc-0a3df104a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class MLP_TORCH(nn.Module):\n",
    "    def __init__(self, layer_sizes, activations = []):\n",
    "        super(MLP_TORCH, self).__init__()\n",
    "        layers = []\n",
    "        self.activations = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if(i < len(activations)):\n",
    "                self.activations.append(None if activations[i] == None else activations[i])\n",
    "            else:\n",
    "                self.activations.append(None)\n",
    "        self.network = nn.ModuleList(layers)\n",
    "        \n",
    "    def get_act_func(self, name):\n",
    "        if(name == \"tanh\"):\n",
    "            return nn.Tanh\n",
    "        if(name == \"relu\"):\n",
    "            return nn.ReLU\n",
    "        if(name == \"sigmoid\"):\n",
    "            return nn.Sigmoid\n",
    "        if(name == \"softmax\"):\n",
    "            return nn.Softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            x = layer(x)\n",
    "            if self.activations[idx]:\n",
    "                act_func = self.get_act_func(self.activations[idx])()\n",
    "                x = act_func(x)\n",
    "        return x\n",
    "\n",
    "    def set_parameters(self, weights, biases):\n",
    "        with torch.no_grad():\n",
    "            for idx, layer in enumerate(self.network):\n",
    "                layer.weight = nn.Parameter(torch.tensor(weights[idx], dtype=torch.float32).T)\n",
    "                layer.bias = nn.Parameter(torch.tensor(biases[idx], dtype=torch.float32))\n",
    "                \n",
    "    def get_parameters(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            weights.append(layer.weight)\n",
    "            biases.append(layer.bias)\n",
    "        return weights, biases\n",
    "\n",
    "def get_err_func(name):\n",
    "    if(name == \"mse\"):\n",
    "        return nn.MSELoss\n",
    "\n",
    "def compare_weights_and_biases_values(mlp_torch, mlp):\n",
    "    e_weights, e_biases = mlp_torch.get_parameters()\n",
    "    weights, biases = mlp.get_parameters()\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        expected = e_weights[i].T\n",
    "        current = torch.tensor(weights[i].data)\n",
    "        assert torch.allclose(expected, current, rtol=1e-4, atol=1e-4), \\\n",
    "            f\"Weights mismatch in layer {i}:\\nExpected: {expected}\\nProvided: {current}\"\n",
    "    for i in range(len(biases)):\n",
    "        expected = e_biases[i].flatten()\n",
    "        current = torch.tensor(biases[i].data).flatten()\n",
    "        assert torch.allclose(expected, current, rtol=1e-4, atol=1e-4), \\\n",
    "            f\"Biases mismatch in layer {i}:\\nExpected: {expected}\\nProvided: {current}\"\n",
    "\n",
    "    #print(\"Parameter values Match\")\n",
    "        \n",
    "def compare_weights_and_biases_grads(mlp_torch, mlp):\n",
    "    e_weights, e_biases = mlp_torch.get_parameters()\n",
    "    weights, biases = mlp.get_parameters()\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        expected = e_weights[i].grad.T\n",
    "        current = torch.tensor(weights[i].grad)\n",
    "        assert torch.allclose(expected, current, rtol=1e-4, atol=1e-4), \\\n",
    "            f\"Weights grads mismatch in layer {i}:\\nExpected: {expected}\\nProvided: {current}\"\n",
    "\n",
    "    for i in range(len(biases)):\n",
    "        expected = e_biases[i].grad.flatten()\n",
    "        current = torch.tensor(biases[i].grad).flatten()\n",
    "        assert torch.allclose(expected, current, rtol=1e-4, atol=1e-4), \\\n",
    "            f\"Biases grads mismatch in layer {i}:\\nExpected: {expected}\\nProvided: {current}\"\n",
    "\n",
    "    # print(\"Parameter grads Match\")\n",
    "\n",
    "def compare_outputs(o1,o2,i):\n",
    "    if o1.dtype != torch.float32:\n",
    "        o1 = o1.float()\n",
    "    if o2.dtype != torch.float32:\n",
    "        o2 = o2.float()\n",
    "    assert torch.allclose(o1, o2, rtol=1e-4, atol=1e-4), \\\n",
    "        f\"Output mismatch at batch starting index {i}:\\nMLP Output: {o1}\\nTorch Output: {o2}\"\n",
    "    #print(f\"Outputs match for this batch {i}\")\n",
    "\n",
    "def compare_mlp(layer_sizes, x, y, batch_size, learning_rate, epochs):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    activations = [\"tanh\", \"relu\", \"softmax\"]\n",
    "    err_func = \"mse\"\n",
    "    for l in range(len(layer_sizes) - 1):\n",
    "        weights.append([[random.uniform(-1, 1) for _ in range(layer_sizes[l + 1])] for _ in range(layer_sizes[l])])\n",
    "        biases.append([random.uniform(-1, 1) for _ in range(layer_sizes[l + 1])])\n",
    "    mlp = MLP(layer_sizes, activations)\n",
    "    mlp.set_parameters(weights, biases)\n",
    "\n",
    "    mlp_torch = MLP_TORCH(layer_sizes, activations)\n",
    "    mlp_torch.set_parameters(weights, biases)\n",
    "\n",
    "    compare_weights_and_biases_values(mlp_torch, mlp)\n",
    "\n",
    "    optimizer = torch.optim.SGD(mlp_torch.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "\n",
    "            mlp_output = mlp.forward(Tensor(x_batch))\n",
    "            \n",
    "            torch_output = mlp_torch.forward(torch.tensor(x_batch))\n",
    "            \n",
    "            compare_outputs(torch.tensor(mlp_output.data), torch_output.data, i)\n",
    "\n",
    "            torch_loss_func = get_err_func(err_func)()\n",
    "            torch_loss = torch_loss_func(torch_output, torch.tensor(y_batch))\n",
    "            loss = err_functions[err_func](mlp_output, Tensor(y_batch))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch_loss.backward()\n",
    "            \n",
    "            mlp.step(learning_rate)\n",
    "            optimizer.step()\n",
    "            compare_weights_and_biases_grads(mlp_torch, mlp)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            mlp.zero_grad()\n",
    "            #print(f\"Epoch {epoch + 1}/{epochs} Test Passed\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd3d47b-7662-4009-b140-30a0c79893cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_test():\n",
    "    input_size = random.randint(2, 16)\n",
    "    output_size = random.randint(2, 16)\n",
    "    hidden_layers = [random.randint(1,4) for _ in range(random.randint(1,16))]\n",
    "    layers = [input_size] + hidden_layers + [output_size]\n",
    "    record_num = random.randint(2, 1024)\n",
    "    batch_size = random.randint(1, record_num)\n",
    "    learning_rate = random.uniform(0.0001, 0.01)\n",
    "    epochs = random.randint(1, 100)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(record_num):\n",
    "        x.append([random.uniform(-5,5) for _ in range(input_size)])\n",
    "        y.append([random.uniform(-5,5) for _ in range(output_size)])\n",
    "    print(f\"Layers: {layers}\")\n",
    "    print(f\"Record number: {record_num}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    compare_mlp(layers,x,y, batch_size, learning_rate, epochs)\n",
    "    print(\"Passed\")\n",
    "    print(\"---------------------------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf2efa2-66c1-4cfa-b3be-f2db6304b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: [13, 2, 1, 2, 3, 3, 4, 3, 4, 4, 4, 1, 2, 14]\n",
      "Record number: 91\n",
      "Batch Size: 82\n",
      "Learning Rate: 0.006630281022362285\n",
      "Epochs: 96\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 1, 2, 2, 2, 2, 3, 14]\n",
      "Record number: 95\n",
      "Batch Size: 88\n",
      "Learning Rate: 0.005268098812423796\n",
      "Epochs: 30\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [12, 1, 1, 3, 1, 1, 4, 1, 1, 3, 1, 1, 4, 3, 3, 9]\n",
      "Record number: 985\n",
      "Batch Size: 661\n",
      "Learning Rate: 0.008890883129914415\n",
      "Epochs: 86\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 4, 1, 3, 1, 4, 4, 4, 3, 1, 1, 16]\n",
      "Record number: 758\n",
      "Batch Size: 665\n",
      "Learning Rate: 0.00690449055939041\n",
      "Epochs: 42\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 1, 4, 3, 4, 2, 1, 4, 3, 1, 1, 7]\n",
      "Record number: 516\n",
      "Batch Size: 29\n",
      "Learning Rate: 0.0036276985532656585\n",
      "Epochs: 90\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 1, 1, 2, 3, 1, 2, 4, 2, 3, 4, 2, 3, 2, 4, 4, 4]\n",
      "Record number: 838\n",
      "Batch Size: 821\n",
      "Learning Rate: 0.007560623132638472\n",
      "Epochs: 94\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 4, 3, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 3, 1, 1, 13]\n",
      "Record number: 545\n",
      "Batch Size: 194\n",
      "Learning Rate: 0.0041520260070109045\n",
      "Epochs: 67\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [7, 4, 4, 4, 3, 4, 4, 3, 4, 2, 3, 1, 5]\n",
      "Record number: 229\n",
      "Batch Size: 107\n",
      "Learning Rate: 0.0008454132853900107\n",
      "Epochs: 44\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 3, 4, 3, 1, 1, 1, 2, 4, 3, 1, 1, 4, 1, 4, 3, 1, 2]\n",
      "Record number: 56\n",
      "Batch Size: 2\n",
      "Learning Rate: 0.0029279703785337887\n",
      "Epochs: 41\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 1, 11]\n",
      "Record number: 233\n",
      "Batch Size: 69\n",
      "Learning Rate: 0.004814738468201834\n",
      "Epochs: 65\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 2, 1, 4, 15]\n",
      "Record number: 886\n",
      "Batch Size: 164\n",
      "Learning Rate: 0.003154343113566967\n",
      "Epochs: 75\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [16, 2, 1, 2, 1, 2, 3, 1, 3, 2, 1, 3, 4, 3, 7]\n",
      "Record number: 47\n",
      "Batch Size: 21\n",
      "Learning Rate: 0.005576611817275323\n",
      "Epochs: 5\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 2, 2, 4, 3, 4, 3, 1, 3, 1, 4, 1, 11]\n",
      "Record number: 17\n",
      "Batch Size: 12\n",
      "Learning Rate: 0.008767258827457472\n",
      "Epochs: 100\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 3, 2, 2, 1, 4, 2, 2, 3, 4, 4, 2, 2, 1, 2, 4, 4, 9]\n",
      "Record number: 64\n",
      "Batch Size: 23\n",
      "Learning Rate: 0.003297182705506731\n",
      "Epochs: 71\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [2, 2, 15]\n",
      "Record number: 398\n",
      "Batch Size: 238\n",
      "Learning Rate: 0.004563690341607995\n",
      "Epochs: 77\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 4, 4, 3, 10]\n",
      "Record number: 915\n",
      "Batch Size: 269\n",
      "Learning Rate: 0.008313005683148059\n",
      "Epochs: 73\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 1, 1, 4, 4, 3, 4, 2]\n",
      "Record number: 149\n",
      "Batch Size: 39\n",
      "Learning Rate: 0.0019329667596518984\n",
      "Epochs: 69\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [13, 2, 2, 1, 1, 2, 2, 3, 3, 1, 4, 2, 4, 1, 2, 4]\n",
      "Record number: 383\n",
      "Batch Size: 44\n",
      "Learning Rate: 0.0033956487568573414\n",
      "Epochs: 33\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [8, 3, 1, 2, 4, 4, 2, 2, 3, 3, 4, 1, 4, 2, 4, 1, 6]\n",
      "Record number: 425\n",
      "Batch Size: 420\n",
      "Learning Rate: 0.005134604400758288\n",
      "Epochs: 66\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [3, 3, 4, 1, 4, 2, 1, 3, 2, 1, 4, 4, 2, 4, 1, 4]\n",
      "Record number: 678\n",
      "Batch Size: 164\n",
      "Learning Rate: 0.008896736948953769\n",
      "Epochs: 26\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 3, 1, 3, 3, 2, 2, 4, 4, 4, 4, 4, 1, 10]\n",
      "Record number: 279\n",
      "Batch Size: 190\n",
      "Learning Rate: 0.0012678248715152294\n",
      "Epochs: 83\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 4, 3, 4, 13]\n",
      "Record number: 233\n",
      "Batch Size: 111\n",
      "Learning Rate: 0.00921659180662128\n",
      "Epochs: 41\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [5, 3, 3, 3, 2, 4, 2, 4, 15]\n",
      "Record number: 106\n",
      "Batch Size: 24\n",
      "Learning Rate: 0.00468396408288469\n",
      "Epochs: 75\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 1, 1, 4, 1, 3, 2, 13]\n",
      "Record number: 631\n",
      "Batch Size: 53\n",
      "Learning Rate: 0.0009781874964948038\n",
      "Epochs: 34\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [8, 1, 3, 1, 1, 1, 4, 2, 4, 4, 3, 2, 3, 3]\n",
      "Record number: 444\n",
      "Batch Size: 22\n",
      "Learning Rate: 0.0016655219336052982\n",
      "Epochs: 24\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [13, 2, 1, 3, 3, 3, 4, 3, 4, 4, 2, 4, 2, 2, 1, 16]\n",
      "Record number: 924\n",
      "Batch Size: 740\n",
      "Learning Rate: 0.004537245187092212\n",
      "Epochs: 74\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [2, 3, 4, 4, 5]\n",
      "Record number: 607\n",
      "Batch Size: 520\n",
      "Learning Rate: 0.000402190633511324\n",
      "Epochs: 48\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 1, 2, 2, 4, 1, 1, 4, 2]\n",
      "Record number: 394\n",
      "Batch Size: 95\n",
      "Learning Rate: 0.00663048207831097\n",
      "Epochs: 78\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 3, 3, 2, 4, 2, 8]\n",
      "Record number: 904\n",
      "Batch Size: 584\n",
      "Learning Rate: 0.0055652134760568\n",
      "Epochs: 55\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 4, 3, 1, 3, 9]\n",
      "Record number: 104\n",
      "Batch Size: 50\n",
      "Learning Rate: 0.003028166131306348\n",
      "Epochs: 36\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 2, 2, 1, 1, 4, 2, 12]\n",
      "Record number: 743\n",
      "Batch Size: 530\n",
      "Learning Rate: 0.0011809054669165233\n",
      "Epochs: 27\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 3, 1, 4, 1, 2, 2, 3, 3, 4, 2, 3, 4, 1, 8]\n",
      "Record number: 506\n",
      "Batch Size: 112\n",
      "Learning Rate: 0.0014964857055166094\n",
      "Epochs: 45\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [7, 1, 1, 1, 3, 1, 2, 3, 3, 3, 4, 4, 4, 3, 3, 15]\n",
      "Record number: 814\n",
      "Batch Size: 100\n",
      "Learning Rate: 0.004965942593094058\n",
      "Epochs: 41\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 3, 13]\n",
      "Record number: 616\n",
      "Batch Size: 600\n",
      "Learning Rate: 0.0015402571770055462\n",
      "Epochs: 4\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 2, 2, 2, 1, 1, 1, 13]\n",
      "Record number: 845\n",
      "Batch Size: 614\n",
      "Learning Rate: 0.009805623046697026\n",
      "Epochs: 35\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [13, 1, 3, 4, 1, 3, 2, 4, 3, 4, 2, 4, 4, 3, 2, 6]\n",
      "Record number: 979\n",
      "Batch Size: 455\n",
      "Learning Rate: 0.00846675311959148\n",
      "Epochs: 75\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [16, 2, 2, 2, 4, 2, 3, 11]\n",
      "Record number: 895\n",
      "Batch Size: 250\n",
      "Learning Rate: 0.00769276870378266\n",
      "Epochs: 46\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [12, 4, 3, 1, 1, 1, 3, 1, 2, 1, 3, 2, 2, 3, 4, 6]\n",
      "Record number: 208\n",
      "Batch Size: 6\n",
      "Learning Rate: 0.009676814169526494\n",
      "Epochs: 59\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 4, 2, 2, 2, 3, 16]\n",
      "Record number: 27\n",
      "Batch Size: 17\n",
      "Learning Rate: 0.0018797750597395134\n",
      "Epochs: 58\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [13, 2, 2, 1, 4, 1, 1, 2, 2, 15]\n",
      "Record number: 581\n",
      "Batch Size: 380\n",
      "Learning Rate: 0.00018681516006851785\n",
      "Epochs: 59\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 4, 4, 4, 3, 3, 3, 1, 2, 3, 3, 1, 13]\n",
      "Record number: 268\n",
      "Batch Size: 160\n",
      "Learning Rate: 0.0015990969877907543\n",
      "Epochs: 85\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 3, 1, 4, 3, 4]\n",
      "Record number: 118\n",
      "Batch Size: 50\n",
      "Learning Rate: 0.006226100014242115\n",
      "Epochs: 50\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [15, 3, 4, 2, 4, 4, 4, 1, 3, 7]\n",
      "Record number: 895\n",
      "Batch Size: 767\n",
      "Learning Rate: 0.005815453606856798\n",
      "Epochs: 82\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 1, 3, 12]\n",
      "Record number: 449\n",
      "Batch Size: 379\n",
      "Learning Rate: 0.0031699384874790096\n",
      "Epochs: 80\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [8, 1, 1, 1, 4, 3, 4, 1, 3, 3, 3, 2, 4, 4, 1, 15]\n",
      "Record number: 463\n",
      "Batch Size: 191\n",
      "Learning Rate: 0.0027341045485763603\n",
      "Epochs: 55\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [8, 2, 4, 1, 3, 2, 3, 2, 1, 4, 10]\n",
      "Record number: 206\n",
      "Batch Size: 151\n",
      "Learning Rate: 0.008252431383694617\n",
      "Epochs: 33\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 2, 4, 3, 1, 2, 4, 4, 1, 1, 2, 1, 3, 9]\n",
      "Record number: 719\n",
      "Batch Size: 295\n",
      "Learning Rate: 0.0005699830812807752\n",
      "Epochs: 4\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 1, 4, 4, 1, 3, 4, 4, 10]\n",
      "Record number: 171\n",
      "Batch Size: 62\n",
      "Learning Rate: 0.002100860625650439\n",
      "Epochs: 9\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [13, 1, 4, 2, 1, 4, 16]\n",
      "Record number: 1010\n",
      "Batch Size: 143\n",
      "Learning Rate: 0.00581869294116986\n",
      "Epochs: 25\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 4, 3, 4, 3, 4, 2, 2, 4, 3, 16]\n",
      "Record number: 865\n",
      "Batch Size: 737\n",
      "Learning Rate: 0.007889370647979581\n",
      "Epochs: 95\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [12, 2, 3, 2, 3, 4, 3, 2, 4, 2, 2, 4, 16]\n",
      "Record number: 166\n",
      "Batch Size: 128\n",
      "Learning Rate: 0.005985624885518978\n",
      "Epochs: 38\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [4, 2, 3, 4, 2, 3, 1, 1, 1, 2, 3, 1, 2, 4, 2, 3, 4]\n",
      "Record number: 997\n",
      "Batch Size: 93\n",
      "Learning Rate: 0.0006468691680847208\n",
      "Epochs: 45\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [3, 1, 3, 3, 3, 2, 2, 4, 1, 2, 2, 1, 1, 2]\n",
      "Record number: 500\n",
      "Batch Size: 140\n",
      "Learning Rate: 0.0038479829331875587\n",
      "Epochs: 8\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 1, 1, 4, 2, 2]\n",
      "Record number: 285\n",
      "Batch Size: 243\n",
      "Learning Rate: 0.004375273670929478\n",
      "Epochs: 44\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 3, 2, 2, 4, 2, 4, 3, 3, 3, 1, 4, 4, 5]\n",
      "Record number: 545\n",
      "Batch Size: 18\n",
      "Learning Rate: 0.005227426720876414\n",
      "Epochs: 27\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 1, 1, 4, 1, 4, 4, 3, 4, 4, 4, 4, 3, 3, 5]\n",
      "Record number: 269\n",
      "Batch Size: 86\n",
      "Learning Rate: 0.0008464463996062358\n",
      "Epochs: 66\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [8, 4, 1, 2, 3, 2, 3, 4, 2]\n",
      "Record number: 836\n",
      "Batch Size: 74\n",
      "Learning Rate: 0.009337495814999892\n",
      "Epochs: 85\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [12, 1, 2, 3, 1, 1, 1, 4, 4, 2]\n",
      "Record number: 917\n",
      "Batch Size: 828\n",
      "Learning Rate: 0.008334095012072842\n",
      "Epochs: 86\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 1, 4, 1, 2, 3, 3, 1, 1, 3, 3, 2, 1, 2, 2, 4, 1, 9]\n",
      "Record number: 172\n",
      "Batch Size: 148\n",
      "Learning Rate: 0.006305485794933123\n",
      "Epochs: 89\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 1, 2, 4, 1, 3, 2, 1, 2, 2, 4, 2, 15]\n",
      "Record number: 274\n",
      "Batch Size: 148\n",
      "Learning Rate: 0.00023842831471810667\n",
      "Epochs: 80\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 2, 1, 4, 1, 2, 2, 2, 4, 4, 2, 2, 3, 3, 1, 1, 1, 3]\n",
      "Record number: 750\n",
      "Batch Size: 480\n",
      "Learning Rate: 0.008259444369191746\n",
      "Epochs: 3\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 1, 7]\n",
      "Record number: 135\n",
      "Batch Size: 74\n",
      "Learning Rate: 0.007383166080419885\n",
      "Epochs: 75\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [3, 2, 1, 8]\n",
      "Record number: 587\n",
      "Batch Size: 159\n",
      "Learning Rate: 0.00039859012122501705\n",
      "Epochs: 39\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 3, 16]\n",
      "Record number: 993\n",
      "Batch Size: 215\n",
      "Learning Rate: 0.006357762322547301\n",
      "Epochs: 40\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [7, 2, 3, 4, 2, 3, 2, 2, 4, 3, 4, 2, 3, 11]\n",
      "Record number: 819\n",
      "Batch Size: 164\n",
      "Learning Rate: 0.0030165385189418296\n",
      "Epochs: 86\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 1, 1, 9]\n",
      "Record number: 211\n",
      "Batch Size: 143\n",
      "Learning Rate: 0.006886788550835872\n",
      "Epochs: 28\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [5, 2, 2, 3, 13]\n",
      "Record number: 909\n",
      "Batch Size: 30\n",
      "Learning Rate: 0.004999516551155579\n",
      "Epochs: 27\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 2, 2, 3, 4, 3, 3, 4, 4, 7]\n",
      "Record number: 55\n",
      "Batch Size: 46\n",
      "Learning Rate: 0.0017472464184788276\n",
      "Epochs: 94\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 2, 4, 1, 1, 16]\n",
      "Record number: 594\n",
      "Batch Size: 134\n",
      "Learning Rate: 0.009226347057719355\n",
      "Epochs: 40\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [4, 3, 1, 1, 2, 4, 4, 3, 3, 4, 2]\n",
      "Record number: 137\n",
      "Batch Size: 77\n",
      "Learning Rate: 0.0020175042782254455\n",
      "Epochs: 34\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 4, 3, 4, 2, 3, 4, 2, 3, 4, 1, 3, 14]\n",
      "Record number: 157\n",
      "Batch Size: 90\n",
      "Learning Rate: 0.0008619916804578145\n",
      "Epochs: 60\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 1, 2, 2, 2, 4, 4, 4]\n",
      "Record number: 696\n",
      "Batch Size: 514\n",
      "Learning Rate: 0.002780992936333627\n",
      "Epochs: 42\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [13, 2, 1, 2, 2, 2, 2, 3, 1, 3, 4, 4, 3, 1, 4, 6]\n",
      "Record number: 961\n",
      "Batch Size: 410\n",
      "Learning Rate: 0.009389884115936241\n",
      "Epochs: 18\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [16, 1, 4, 2, 3, 2, 3, 4, 2, 3, 1, 2, 1, 3, 2, 7]\n",
      "Record number: 607\n",
      "Batch Size: 211\n",
      "Learning Rate: 0.003077972224080438\n",
      "Epochs: 69\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [15, 3, 1, 1, 2, 2, 2, 2, 1, 3, 2, 2, 11]\n",
      "Record number: 829\n",
      "Batch Size: 771\n",
      "Learning Rate: 0.003854461152559069\n",
      "Epochs: 2\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [8, 4, 1, 16]\n",
      "Record number: 29\n",
      "Batch Size: 22\n",
      "Learning Rate: 0.0006564795807770686\n",
      "Epochs: 23\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [4, 2, 3, 1, 4, 4, 2, 1, 2, 3, 3, 2, 1, 3, 12]\n",
      "Record number: 390\n",
      "Batch Size: 368\n",
      "Learning Rate: 0.0046770600227821375\n",
      "Epochs: 82\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [12, 4, 2, 4, 2, 4, 3, 1, 4, 1, 1, 2, 1, 1, 3, 4, 4, 6]\n",
      "Record number: 659\n",
      "Batch Size: 534\n",
      "Learning Rate: 0.0008321441624034713\n",
      "Epochs: 9\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [8, 2, 4, 1, 2, 3, 3, 1, 3, 1, 4, 1, 2, 1, 1, 2]\n",
      "Record number: 414\n",
      "Batch Size: 192\n",
      "Learning Rate: 0.005619388182936272\n",
      "Epochs: 21\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 4, 4, 3, 2, 3, 2, 1, 1, 4, 1, 1, 4, 11]\n",
      "Record number: 192\n",
      "Batch Size: 174\n",
      "Learning Rate: 0.0011456394734750835\n",
      "Epochs: 28\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 2, 2, 3, 2, 2, 8]\n",
      "Record number: 806\n",
      "Batch Size: 190\n",
      "Learning Rate: 0.004894050495386803\n",
      "Epochs: 100\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [11, 4, 1, 2, 11]\n",
      "Record number: 356\n",
      "Batch Size: 162\n",
      "Learning Rate: 0.004444491537895971\n",
      "Epochs: 76\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 2, 3, 3, 3, 4, 5]\n",
      "Record number: 370\n",
      "Batch Size: 299\n",
      "Learning Rate: 0.004529005571989808\n",
      "Epochs: 92\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 4, 2, 3, 1, 2, 1, 3, 2, 2, 3, 3, 2]\n",
      "Record number: 411\n",
      "Batch Size: 250\n",
      "Learning Rate: 0.005965624377165606\n",
      "Epochs: 77\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [12, 2, 4, 2, 2, 2, 2, 1, 4, 1, 3, 1, 4, 3, 4, 4, 5]\n",
      "Record number: 899\n",
      "Batch Size: 737\n",
      "Learning Rate: 0.006272671184539531\n",
      "Epochs: 16\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 4, 3, 4, 4, 4, 3, 2, 1, 2, 15]\n",
      "Record number: 128\n",
      "Batch Size: 63\n",
      "Learning Rate: 0.004062515029745266\n",
      "Epochs: 20\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [15, 4, 4, 1, 3, 11]\n",
      "Record number: 692\n",
      "Batch Size: 551\n",
      "Learning Rate: 0.000859255766276288\n",
      "Epochs: 91\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [2, 2, 4, 1, 4, 1, 4, 3, 1, 3, 4]\n",
      "Record number: 759\n",
      "Batch Size: 38\n",
      "Learning Rate: 0.0018718982751867992\n",
      "Epochs: 79\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 4, 4, 1, 1, 2, 4, 1, 4, 3, 2, 3, 4, 3, 3, 1, 8]\n",
      "Record number: 899\n",
      "Batch Size: 4\n",
      "Learning Rate: 0.00505628564433382\n",
      "Epochs: 70\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [14, 4, 16]\n",
      "Record number: 239\n",
      "Batch Size: 8\n",
      "Learning Rate: 0.003737946787461627\n",
      "Epochs: 55\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [7, 1, 4, 1, 2, 6]\n",
      "Record number: 674\n",
      "Batch Size: 114\n",
      "Learning Rate: 0.002535776883433126\n",
      "Epochs: 81\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [13, 2, 2, 2, 1, 4, 1, 4, 2, 2, 15]\n",
      "Record number: 691\n",
      "Batch Size: 199\n",
      "Learning Rate: 0.0017942286149501918\n",
      "Epochs: 100\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [13, 3, 8]\n",
      "Record number: 456\n",
      "Batch Size: 450\n",
      "Learning Rate: 0.007075427713712845\n",
      "Epochs: 94\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 2, 3, 3, 4, 3, 3, 4, 1, 2, 3, 2, 4, 1, 1, 7]\n",
      "Record number: 223\n",
      "Batch Size: 137\n",
      "Learning Rate: 0.006502961557374824\n",
      "Epochs: 69\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [7, 3, 1, 4, 3, 4]\n",
      "Record number: 691\n",
      "Batch Size: 87\n",
      "Learning Rate: 0.00740779822293981\n",
      "Epochs: 30\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [10, 1, 2, 3, 1, 3, 8]\n",
      "Record number: 792\n",
      "Batch Size: 517\n",
      "Learning Rate: 0.0011557920386098114\n",
      "Epochs: 41\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [9, 4, 4, 3, 1, 3, 4, 4, 4, 4, 4, 1, 3, 12]\n",
      "Record number: 144\n",
      "Batch Size: 76\n",
      "Learning Rate: 0.009253883971156151\n",
      "Epochs: 9\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [6, 3, 1, 4, 3, 3, 1, 4, 2, 9]\n",
      "Record number: 643\n",
      "Batch Size: 504\n",
      "Learning Rate: 0.002026018780943209\n",
      "Epochs: 43\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [8, 2, 3, 3, 1, 3, 7]\n",
      "Record number: 445\n",
      "Batch Size: 408\n",
      "Learning Rate: 0.005686355615038214\n",
      "Epochs: 92\n",
      "Passed\n",
      "---------------------------------\n",
      "Layers: [4, 1, 1, 1, 4, 1, 3, 3, 3]\n",
      "Record number: 821\n",
      "Batch Size: 402\n",
      "Learning Rate: 0.002047463646971002\n",
      "Epochs: 21\n",
      "Passed\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    random_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797c164d-d649-433c-aa41-8ae5851a31c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Passed\n",
      "1 Passed\n",
      "2 Passed\n",
      "3 Passed\n",
      "4 Passed\n",
      "5 Passed\n",
      "6 Passed\n",
      "7 Passed\n",
      "8 Passed\n",
      "9 Passed\n",
      "10 Passed\n",
      "11 Passed\n",
      "12 Passed\n",
      "13 Passed\n",
      "14 Passed\n",
      "15 Passed\n",
      "16 Passed\n",
      "17 Passed\n",
      "18 Passed\n",
      "19 Passed\n",
      "20 Passed\n",
      "21 Passed\n",
      "22 Passed\n",
      "23 Passed\n",
      "24 Passed\n",
      "25 Passed\n",
      "26 Passed\n",
      "27 Passed\n",
      "28 Passed\n",
      "29 Passed\n",
      "30 Passed\n",
      "31 Passed\n",
      "32 Passed\n",
      "33 Passed\n",
      "34 Passed\n",
      "35 Passed\n",
      "36 Passed\n",
      "37 Passed\n",
      "38 Passed\n",
      "39 Passed\n",
      "40 Passed\n",
      "41 Passed\n",
      "42 Passed\n",
      "43 Passed\n",
      "44 Passed\n",
      "45 Passed\n",
      "46 Passed\n",
      "47 Passed\n",
      "48 Passed\n",
      "49 Passed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This test case failed once. Can't reproduce. Probably caused by how pytorch handles floating point arithmetics\n",
    "AssertionError: Weights grads mismatch in layer 0:\n",
    "Expected: tensor([[-0.0052, -0.0188],\n",
    "        [ 0.0107, -0.0067],\n",
    "        [ 0.0038, -0.0006],\n",
    "        [-0.0027, -0.0385],\n",
    "        [ 0.0029, -0.0005],\n",
    "        [-0.0034,  0.0159],\n",
    "        [-0.0055, -0.0004],\n",
    "        [-0.0142, -0.0040],\n",
    "        [-0.0067, -0.0030],\n",
    "        [ 0.0100, -0.0203]])\n",
    "Provided: tensor([[-0.0054, -0.0188],\n",
    "        [ 0.0104, -0.0067],\n",
    "        [ 0.0042, -0.0006],\n",
    "        [-0.0028, -0.0385],\n",
    "        [ 0.0030, -0.0005],\n",
    "        [-0.0037,  0.0159],\n",
    "        [-0.0052, -0.0004],\n",
    "        [-0.0140, -0.0040],\n",
    "        [-0.0067, -0.0030],\n",
    "        [ 0.0102, -0.0203]])\n",
    "\"\"\"\n",
    "\n",
    "layers =[10, 2, 1, 3, 2]\n",
    "record_num =  751\n",
    "batch_size= 586\n",
    "learning_rate =  0.00284773637316031\n",
    "epochs = 73\n",
    "x = []\n",
    "y = []\n",
    "for _ in range(record_num):\n",
    "    x.append([random.uniform(-5,5) for _ in range(10)])\n",
    "    y.append([random.uniform(-5,5) for _ in range(2)])\n",
    "for i in range(50):\n",
    "    compare_mlp(layers,x,y, batch_size, learning_rate, epochs)\n",
    "    print(i , \"Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb595ee-c881-4270-b21f-742bf84aaadc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
